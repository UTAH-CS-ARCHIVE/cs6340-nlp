### **[Lecture Note] Recurrent Neural Networks (RNN)**

> Written by Hongseo Jang

#### **1. The Structure of RNNs for Sequence Data**

Standard feedforward neural networks are designed to handle fixed-size inputs and do not have a mechanism to model temporal dependencies. They lack "memory." This makes them unsuitable for sequence data like text, speech, or time series, where the order of elements is crucial.

**Recurrent Neural Networks (RNNs)** are a class of neural networks specifically designed to address this. The defining feature of an RNN is its internal loop, which allows information to persist. At each time step, the network's output is influenced not only by the current input but also by a "hidden state" that contains information from all previous time steps.

This "memory" is captured in the **hidden state**, denoted as $h_t$. The hidden state at time step *t* is a function of the hidden state at the previous time step, $h_{t-1}$, and the current input, $x_t$. This creates a recurrent relationship that allows the network to maintain a summary of the past sequence.

<br>

#### **2. The RNN Forward Pass: Information Flow over Time**

The core of an RNN is its recurrence formula, which defines how the hidden state is updated at each time step.

The mathematical relationship is given by:

$$
h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)
$$

Let's break down this equation:
-   $h_t$: The new hidden state at time step *t*. This vector acts as the network's memory.
-   $h_{t-1}$: The hidden state from the previous time step *t-1*.
-   $x_t$: The input vector at the current time step *t*.
-   $W_{hh}$: The weight matrix for the recurrent connection (hidden-to-hidden).
-   $W_{xh}$: The weight matrix for the input connection (input-to-hidden).
-   $b_h$: A bias vector for the hidden layer.
-   $f$: A non-linear activation function, typically `tanh` or `ReLU`.

Crucially, the weight matrices ($W_{hh}, W_{xh}$) and the bias ($b_h$) are **shared across all time steps**. This means the network applies the same transformation rules at every point in the sequence, allowing it to generalize across different sequence lengths.

Once the hidden state is computed, an output for the current time step, $\hat{y}_t$, can be generated:
$$
\hat{y}_t = g(W_{hy}h_t + b_y)
$$
Here, $W_{hy}$ and $b_y$ are the weights and bias for the output layer, and *g* is an activation function (e.g., softmax for classification).

<br>

#### **3. Backpropagation Through Time (BPTT)**

To train an RNN, we need to calculate the gradient of a total loss function with respect to the shared parameters. The total loss *L* is typically the sum of the losses at each time step, $L_t$.

**Backpropagation Through Time (BPTT)** is the algorithm used to train RNNs. It is essentially the standard backpropagation algorithm applied to the "unrolled" version of the RNN. Unrolling the RNN means visualizing the loop as a very deep feedforward network, where each time step is a layer that shares weights with all other layers.

The most critical gradient to understand is for the recurrent weight matrix, $W_{hh}$, as it's involved in the propagation of information through time. Since $W_{hh}$ is used at every step, its total gradient is the sum of its gradients at each time step:
$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial W_{hh}}
$$
To calculate the gradient at a particular time step *t*, we need to use the chain rule. The hidden state $h_t$ depends on $h_{t-1}$, which in turn depends on $h_{t-2}$, and so on. Therefore, an error at time *t* has an influence that propagates backward through the entire sequence.

The gradient of the loss at time *t* with respect to a past hidden state $h_k$ (where $k<t$) is:
$$
\frac{\partial L_t}{\partial h_k} = \frac{\partial L_t}{\partial \hat{y}_t} \frac{\partial \hat{y}_t}{\partial h_t} \frac{\partial h_t}{\partial h_k}
$$
The key term is $\frac{\partial h_t}{\partial h_k}$, which further breaks down via the chain rule:
$$
\frac{\partial h_t}{\partial h_k} = \frac{\partial h_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \cdots \frac{\partial h_{k+1}}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}
$$
This equation shows that the gradient signal from the future (*t*) to the past (*k*) involves a long product of Jacobian matrices, $\frac{\partial h_i}{\partial h_{i-1}}$.

<br>

#### **4. The Vanishing and Exploding Gradient Problem**

The long product of Jacobian matrices in BPTT is the mathematical root of the most significant challenge in training simple RNNs: the vanishing and exploding gradient problem.

Let's analyze the Jacobian matrix term $\frac{\partial h_i}{\partial h_{i-1}}$ from the recurrence relation $h_i = f(W_{hh}h_{i-1} + \dots)$. Its value is:
$$
\frac{\partial h_i}{\partial h_{i-1}} = \text{diag}(f'(z_i))W_{hh}^T
$$
where $z_i$ is the input to the activation function at step *i*.

The product term then becomes:
$$
\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \text{diag}(f'(z_i))W_{hh}^T
$$

The behavior of this product determines the stability of the gradients:
-   **Vanishing Gradients:** If the norm of the Jacobian matrix is consistently less than 1 over many time steps (i.e., $\| \frac{\partial h_i}{\partial h_{i-1}} \| < 1$), the product will shrink exponentially as the time gap ($t-k$) increases. The gradients will approach zero, and the error signal from distant past steps will be effectively lost. This prevents the model from learning long-range dependencies in the data. For example, if the activation function is `tanh`, its derivative is always between 0 and 1, which contributes to shrinking the gradients.
-   **Exploding Gradients:** Conversely, if the norm of the Jacobian matrix is consistently greater than 1 (i.e., $\| \frac{\partial h_i}{\partial h_{i-1}} \| > 1$), the product will grow exponentially. The gradients will become astronomically large, leading to numerical overflow and causing the training process to diverge. This is often easier to detect and can be mitigated with techniques like gradient clipping.

This mathematical instability is the primary reason why simple RNNs struggle with long sequences and motivated the development of more complex architectures like LSTMs and GRUs, which are designed to control the flow of information and gradients through time more effectively.