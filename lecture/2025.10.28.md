### **[Lecture Note] LSTM and GRU (Long Short-Term Memory & Gated Recurrent Unit)**

> Written by Hongseo Jang

#### **1. The Long-Term Dependency Problem in RNNs**

As discussed previously, simple Recurrent Neural Networks (RNNs) suffer from the vanishing and exploding gradient problem. During backpropagation through time, the repeated multiplication of Jacobian matrices causes the gradient signal to either shrink exponentially to zero or grow uncontrollably. The vanishing gradient problem is particularly troublesome, as it prevents the network from learning dependencies between events that are far apart in a sequence. This inability to capture "long-term dependencies" makes simple RNNs ineffective for many real-world tasks.

To address this fundamental issue, more sophisticated recurrent architectures were developed. The two most prominent are the **Long Short-Term Memory (LSTM)** and the **Gated Recurrent Unit (GRU)**. Both introduce **gate mechanisms** to regulate the flow of information and gradients through the network.

<br>

#### **2. Long Short-Term Memory (LSTM)**

An LSTM network introduces a dedicated **cell state**, $C_t$, which acts as a "conveyor belt" of information. The cell state can carry information straight down the entire sequence with only minor linear interactions. This makes it much easier for information from earlier time steps to be preserved.

The flow of information into and out of this cell state is carefully controlled by three specialized gates: the Forget Gate, the Input Gate, and the Output Gate. These gates are composed of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between 0 and 1, describing how much of each component should be let through. A value of 0 means "let nothing through," while a value of 1 means "let everything through."

Let's analyze the process at a time step *t*:

**Step 1: The Forget Gate ($f_t$)**
This gate decides what information to discard from the previous cell state, $C_{t-1}$. It looks at the previous hidden state, $h_{t-1}$, and the current input, $x_t$, and outputs a vector of numbers between 0 and 1 for each number in the cell state $C_{t-1}$.

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

**Step 2: The Input Gate ($i_t$) and Candidate Cell State ($\tilde{C}_t$)**
This two-part step decides what new information to store in the cell state.
First, the input gate's sigmoid layer decides which values we'll update.
Second, a `tanh` layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state.

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

$$
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
$$

**Step 3: Updating the Cell State ($C_t$)**
We now update the old cell state, $C_{t-1}$, into the new cell state, $C_t$. We multiply the old state by $f_t$ (forgetting the things we decided to forget) and add the new candidate values, scaled by $i_t$.

$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$
Here, $\odot$ denotes element-wise multiplication.

**Step 4: The Output Gate ($o_t$) and Hidden State ($h_t$)**
Finally, we decide what our output (the hidden state $h_t$) will be. The output is a filtered version of our cell state.
First, the output gate decides which parts of the cell state we're going to output.
Then, we put the cell state through `tanh` (to squash the values to be between -1 and 1) and multiply it by the output of the sigmoid gate.

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

$$
h_t = o_t \odot \tanh(C_t)
$$

This gating structure, especially the additive interaction in the cell state update, helps gradients to flow unchanged, which is the key to mitigating the vanishing gradient problem.

<br>

#### **3. Gated Recurrent Unit (GRU)**

The Gated Recurrent Unit (GRU) is a more recent and slightly simpler alternative to the LSTM. It combines the forget and input gates into a single **update gate** and also merges the cell state and hidden state. This results in a model with fewer parameters, which can be computationally more efficient.

A GRU has two gates:

**1. Reset Gate ($r_t$):** This gate determines how to combine the new input with the previous memory. Specifically, it decides how much of the previous hidden state to forget.

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
$$

**2. Update Gate ($z_t$):** This gate decides how much of the previous memory to keep around. It's analogous to the forget and input gates of an LSTM, controlling what information from $h_{t-1}$ is kept and what new information from the candidate hidden state is added.

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
$$

The hidden state is then updated as follows:

First, a candidate hidden state, $\tilde{h}_t$, is computed. The reset gate, $r_t$, controls how much influence the previous state $h_{t-1}$ has on this candidate. If a component of $r_t$ is close to 0, the corresponding component of $h_{t-1}$ is ignored.

$$
\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
$$

Finally, the new hidden state, $h_t$, is a convex combination of the previous hidden state, $h_{t-1}$, and the candidate hidden state, $\tilde{h}_t$. The update gate, $z_t$, controls this combination.

$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$

If $z_t$ is close to 1, the new hidden state is almost entirely the candidate state. If it's close to 0, the previous hidden state is mostly preserved. This mechanism allows the GRU to maintain long-term information and makes it robust against the vanishing gradient problem.