### **[Lecture Note] Neural Network Basics and the Backpropagation Algorithm**

> Written by Hongseo Jang

#### **1. Basic Structure of an Artificial Neural Network (ANN)**

An Artificial Neural Network is a computational model inspired by the structure and function of biological neural networks. It consists of interconnected nodes, called **neurons**, organized in layers. For many NLP tasks, a basic feedforward neural network includes:

1.  **Input Layer:** This layer receives the initial data. In NLP, the input is typically a numerical representation of text, such as pre-trained word embeddings (e.g., from Word2Vec) for each word in a sentence.

2.  **Hidden Layers:** These are one or more layers of neurons between the input and output layers. Each neuron in a hidden layer receives inputs from the previous layer, performs a computation, and passes the result to the next layer. The computation within a single neuron involves:
    * A weighted sum of its inputs, plus a **bias** term.
    * An **activation function** applied to this sum. The activation function (e.g., Sigmoid, ReLU, Tanh) introduces non-linearity into the model, allowing it to learn complex patterns that a simple linear model cannot.

3.  **Output Layer:** This is the final layer that produces the network's prediction. The structure of the output layer depends on the task. For text classification, it might have one neuron per class, with a softmax activation function to produce a probability distribution over the classes.

<br>

#### **2. Forward Propagation**

Forward propagation is the process by which input data is passed through the network to produce an output (a prediction). The calculations flow "forward" from the input layer to the output layer.

Let's consider a single neuron. It receives inputs $x_1, x_2, \dots, x_n$ with corresponding weights $w_1, w_2, \dots, w_n$ and a bias *b*.
First, it computes the weighted sum, *z*:
$$
z = \left(\sum_{i=1}^{n} w_i x_i\right) + b
$$
Then, it applies an activation function, $\sigma$, to produce its output, *a*:
$$
a = \sigma(z)
$$
This process is repeated for every neuron in a layer, and then for each subsequent layer. In matrix notation for a full layer (denoted by superscript `[l]`):
-   The input to layer `[l]` is the activation from the previous layer, $A^{[l-1]}$.
-   The weighted sum is calculated as: $Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$
-   The output (activation) of the layer is: $A^{[l]} = \sigma(Z^{[l]})$

This continues until we reach the final output layer, which produces the prediction, $\hat{y}$. We then compare this prediction to the true label, *y*, using a **loss function**, $L(\hat{y}, y)$, which quantifies the model's error.

<br>

#### **3. Backpropagation: Deriving the Gradients**

The goal of training is to adjust the weights (*W*) and biases (*b*) to minimize the loss *L*. To do this using an algorithm like Gradient Descent, we need to compute the **gradient** of the loss function with respect to every parameter in the network. The gradient is a vector of **partial derivatives** (e.g., $\frac{\partial L}{\partial W^{[l]}}$).

**Backpropagation** is an efficient algorithm for computing these gradients. It relies fundamentally on the **chain rule of calculus for multivariable functions**. The core idea is to first compute the error at the output layer and then propagate this error signal "backward" through the network, layer by layer.

Let's derive it for a simple 2-layer network. For simplicity, we'll use squared error loss: $L = \frac{1}{2}(y - \hat{y})^2$. Let the activation function be $\sigma(z)$.

**Network Architecture:**
-   Input: $x$
-   Layer 1 (hidden): $z^{[1]} = W^{[1]}x + b^{[1]}$, $a^{[1]} = \sigma(z^{[1]})$
-   Layer 2 (output): $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$, $\hat{y} = a^{[2]} = \sigma(z^{[2]})$

**Step 1: Gradients for the Output Layer (Layer 2)**

We need to find $\frac{\partial L}{\partial W^{[2]}}$ and $\frac{\partial L}{\partial b^{[2]}}$. We use the chain rule:
$$
\frac{\partial L}{\partial W^{[2]}} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial W^{[2]}}
$$

Let's compute each term:
1.  $\frac{\partial L}{\partial \hat{y}} = (\hat{y} - y)$
2.  $\frac{\partial \hat{y}}{\partial z^{[2]}} = \sigma'(z^{[2]})$ (The derivative of the activation function)
3.  $\frac{\partial z^{[2]}}{\partial W^{[2]}} = a^{[1]}$

Combining these, we get:
$$
\frac{\partial L}{\partial W^{[2]}} = (\hat{y} - y) \sigma'(z^{[2]}) a^{[1]}
$$

Let's define the error term for layer 2 as $\delta^{[2]} = \frac{\partial L}{\partial z^{[2]}} = (\hat{y} - y) \sigma'(z^{[2]})$. Then the gradient for the weight is simply $\delta^{[2]} (a^{[1]})^T$. (Transpose is needed for correct matrix dimensions). Similarly, the gradient for the bias is just $\delta^{[2]}$.

**Step 2: Gradients for the Hidden Layer (Layer 1)**

This is the key step of backpropagation. We need to find $\frac{\partial L}{\partial W^{[1]}}$. The chain of derivatives now extends back through layer 2.
$$
\frac{\partial L}{\partial W^{[1]}} = \frac{\partial L}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial a^{[1]}} \frac{\partial a^{[1]}}{\partial z^{[1]}} \frac{\partial z^{[1]}}{\partial W^{[1]}}
$$

Let's analyze the new terms:
1.  $\frac{\partial L}{\partial z^{[2]}}$ is the error term $\delta^{[2]}$ we already calculated.
2.  $\frac{\partial z^{[2]}}{\partial a^{[1]}}$: Since $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$, this derivative is simply the weight matrix $W^{[2]}$. This is the crucial link: the error from layer 2 is propagated backward, weighted by the connections $W^{[2]}$.
3.  $\frac{\partial a^{[1]}}{\partial z^{[1]}} = \sigma'(z^{[1]})$
4.  $\frac{\partial z^{[1]}}{\partial W^{[1]}} = x$

Combining these, we can define the error term for layer 1, $\delta^{[1]}$:
$$
\delta^{[1]} = \frac{\partial L}{\partial z^{[1]}} = \frac{\partial L}{\partial z^{[2]}} \frac{\partial z^{[2]}}{\partial a^{[1]}} \frac{\partial a^{[1]}}{\partial z^{[1]}} = (W^{[2]})^T \delta^{[2]} \odot \sigma'(z^{[1]})
$$
(Here, $\odot$ denotes element-wise multiplication).

The error at layer 1 is the error from layer 2, propagated back through the weights $W^{[2]}$, and then multiplied by the local gradient of the activation function at layer 1.

Finally, the gradient for the weights of layer 1 is:
$$
\frac{\partial L}{\partial W^{[1]}} = \delta^{[1]} x^T
$$

This process can be generalized to any number of layers. The algorithm computes the error $\delta$ at the output layer and then recursively computes the error for each preceding layer. Once all gradients are known, they are used to update the parameters.