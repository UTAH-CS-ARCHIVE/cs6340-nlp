### **[Lecture Note] Probabilistic Language Models**

> Written by Hongseo Jang

### The Core Problem: Data Sparsity in N-gram Models

Language is vast and creative. The number of possible valid word sequences is infinite. Any text corpus we use for training, no matter how large, will only be a tiny sample of this. This leads to a critical issue for N-gram models called **data sparsity** or the **zero-frequency problem**.

* **Problem Definition:** Many plausible N-grams will not appear in the training corpus. For example, in a bigram model trained on a general text corpus, the bigram "computational linguistics" might be frequent. However, a perfectly valid bigram like "computational gastronomy" might have never appeared.
* **Mathematical Consequence:** The Maximum Likelihood Estimation (MLE) for the probability of an N-gram is calculated as:
    $$
    P_{MLE}(w_n | w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n})}{C(w_{n-N+1}^{n-1})}
    $$
    If the count of the full N-gram, $C(w_{n-N+1}^{n})$, is zero, its probability becomes zero.
* **Practical Impact:** When calculating the probability of an entire sentence (which is the product of the probabilities of its N-grams), if even one N-gram has a probability of zero, the entire sentence's probability becomes zero. This makes the model extremely brittle and unable to generalize to unseen but valid sequences.

**Smoothing** is the general term for techniques that address this problem by adjusting the MLE probabilities. The core idea is to "steal" a bit of probability mass from the N-grams we *have* seen and redistribute it to the N-grams we *haven't* seen.

<br>

### Smoothing Technique 1: Laplace Smoothing (Add-One)

This is the most intuitive and simplest smoothing method, reviewed from our first session.

* **The Idea:** Pretend we have seen every possible N-gram one more time than we actually did. This is achieved by adding one to the numerator (the N-gram count). To keep the probabilities normalized, we add the total number of unique words in our vocabulary, $V$, to the denominator.
* **Formula (for a bigram model):**
    $$
    P_{Laplace}(w_i | w_{i-1}) = \frac{C(w_{i-1}w_i) + 1}{C(w_{i-1}) + V}
    $$
* **Drawback:** While simple, Laplace smoothing is often a poor performer in practice. For large vocabularies (common in NLP), it assigns way too much probability mass to the vast number of unseen events. This overly discounts the counts of the events we actually did see, leading to a distorted probability distribution. A slightly better version is **Add-k smoothing**, where we add a small fractional value $k$ (e.g., 0.01) instead of 1, but this still has fundamental limitations.

<br>

### Smoothing Technique 2: Good-Turing Smoothing

Good-Turing is a more statistically principled approach that estimates the probability of unseen events based on the frequency of events seen only once.

* **The Core Intuition:** The number of things we've seen only once ($N_1$) gives us a good estimate for the total probability mass of things we've never seen. We can generalize this idea: the number of N-grams that appeared $c+1$ times can inform our estimate for the N-grams that appeared $c$ times.
* **Key Terminology:**
    * $c$: The original count or frequency of a given N-gram.
    * $N_c$: The "frequency of frequency," meaning the number of *distinct* N-grams that appear exactly $c$ times in the corpus. For example, if the bigrams "of the" and "in a" both appear 500 times and no other bigram does, then $N_{500} = 2$.
* **The Adjusted Count ($c^*$):** Instead of using the raw count $c$, Good-Turing calculates a smoothed count $c^*$. This is the central formula for the technique:
    $$
    c^* = (c+1) \frac{N_{c+1}}{N_c}
    $$
    This $c^*$ is then used to calculate the probability. For an N-gram with original count $c$, its new probability is $P_{GT} = \frac{c^*}{N}$, where $N$ is the total number of N-gram tokens in the corpus.
* **Handling Unseen Events (c=0):** The total probability mass for all unseen N-grams is calculated as $N_1/N$. This mass is then distributed uniformly among all unseen N-grams.
* **Strengths and Weaknesses:** Good-Turing is statistically motivated and performs much better than Laplace. However, it can be unreliable for large counts of $c$ where $N_c$ or $N_{c+1}$ might be zero or very small and noisy. In practice, it's common to use Good-Turing for low-frequency counts (e.g., $c < 5$) and trust the original MLE estimates for higher counts.

<br>

### Smoothing Technique 3: Kneser-Ney Smoothing

Kneser-Ney is considered the state-of-the-art and most effective smoothing method for N-gram models. It's more complex but is based on a very powerful linguistic intuition.

* **The Core Intuition (Continuation Probability):** The probability of a word appearing in a new context should be proportional to the number of *different contexts* it has appeared in before.
    * Consider the word "Francisco". It almost exclusively follows the word "San".
    * Now consider the word "table". It can follow many words: "on the", "dinner", "round", "periodic", etc.
    * Kneser-Ney's insight is that "table" is a better candidate to appear in a new, unseen context than "Francisco" is, because it is more "promiscuous" in the partners it keeps. This is called the **continuation probability**.

* **The Mechanism (Interpolated Kneser-Ney):** It combines a higher-order model with a lower-order model using interpolation. The key is that the lower-order model is not a standard unigram distribution but is based on this continuation probability.
    1.  **Absolute Discounting:** For every seen N-gram, subtract a small, fixed discount value, $d$ (typically between 0 and 1), from its count. This "saves" some probability mass.
    2.  **Interpolation with a Lower-Order Model:** The final probability is a combination of this discounted higher-order probability and a lower-order distribution, weighted by a term $\lambda$.
        $$
        P_{KN}(w_i | w_{i-1}) = \frac{\max(C(w_{i-1}w_i) - d, 0)}{C(w_{i-1})} + \lambda(w_{i-1}) P_{continuation}(w_i)
        $$
    3.  **The Continuation Probability:** This is the clever part. The $P_{continuation}(w_i)$ term is proportional to the number of *unique word types* that preceded $w_i$ in the training corpus.
        $$
        P_{continuation}(w_i) = \frac{|\{w_{i-1} : C(w_{i-1}w_i) > 0\}|}{\sum_{w_j} |\{w_{j-1} : C(w_{j-1}w_j) > 0\}|}
        $$
        This formula directly captures the intuition of how many different contexts a word can appear in.
    4.  The $\lambda$ term is a normalization constant that distributes the probability mass we saved from the discounting step.

* **Why it works so well:** Kneser-Ney excels because the continuation probability is a much smarter way to handle the backoff (lower-order) distribution compared to just using standard unigram probabilities. It directly addresses the linguistic reality of word distributions.