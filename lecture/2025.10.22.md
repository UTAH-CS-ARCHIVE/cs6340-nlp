### **[Lecture Note] Recurrent Neural Networks (RNN)**

> Written by Hongseo Jang

### Bidirectional RNNs (Bi-RNNs)

**Motivation:**
Consider the task of Named Entity Recognition (NER). In the sentence, "Teddy Roosevelt was a great president," it's easy to identify "Teddy" as part of a person's name. But in the sentence, "I gave my friend a teddy bear," the word "teddy" has a completely different meaning. To correctly classify "Teddy" in the first sentence, we need to see the word "Roosevelt" which comes *after* it. A standard RNN, reading left-to-right, would have to make a decision about "Teddy" without this crucial future context.

Bi-RNNs solve this problem by processing the sequence in both directions, ensuring that the representation for any word is informed by its entire context.

**Architecture:**
A Bi-RNN is not a single complex RNN. Instead, it consists of two independent standard RNNs:
1.  **A Forward RNN:** This network processes the input sequence from left to right (from $t=1$ to $T$). It produces a sequence of *forward hidden states*, $\overrightarrow{h_1}, \overrightarrow{h_2}, \dots, \overrightarrow{h_T}$.
2.  **A Backward RNN:** This network processes the same input sequence but in reverse, from right to left (from $t=T$ to $1$). It produces a sequence of *backward hidden states*, $\overleftarrow{h_1}, \overleftarrow{h_2}, \dots, \overleftarrow{h_T}$.



**Mathematical Formulation:**
For each time step $t$, the hidden states are computed as follows:
* **Forward State:**
    $$
    \overrightarrow{h_t} = f(W_{\overrightarrow{h}x} x_t + W_{\overrightarrow{h}\overrightarrow{h}} \overrightarrow{h}_{t-1} + b_{\overrightarrow{h}})
    $$
* **Backward State:**
    $$
    \overleftarrow{h_t} = f(W_{\overleftarrow{h}x} x_t + W_{\overleftarrow{h}\overleftarrow{h}} \overleftarrow{h}_{t+1} + b_{\overleftarrow{h}})
    $$
    Note that the forward and backward RNNs have their own separate sets of weights.

**Key Concept: Concatenation**
To create a holistic representation at time $t$, we combine the information from both directions. The most common method is to **concatenate** the forward and backward hidden states at that time step:
$$
h_t = [\overrightarrow{h_t} ; \overleftarrow{h_t}]
$$
This combined hidden state $h_t$ now contains a summary of the sequence both before and after the word at position $t$. This rich representation is then passed to the output layer to make a prediction.
$$
\hat{y}_t = g(W_y h_t + b_y)
$$

<br>

### Deep (Stacked) RNNs

**Motivation:**
Just as deep feed-forward networks and CNNs can learn a hierarchy of features (from simple edges to complex objects), we can stack RNN layers to enable the learning of hierarchical temporal features. A single-layer RNN might be forced to learn how to encode low-level syntax and high-level semantics all within the same hidden state representation. A deep RNN can distribute this work across its layers.

* **Layer 1:** Might learn basic grammatical and syntactic features.
* **Layer 2:** Could take the sequence of syntactic features from Layer 1 and learn to identify phrase-level semantic patterns.
* **Layer 3:** Might learn to model discourse-level relationships based on the semantic patterns from Layer 2.

**Architecture:**
In a Deep or Stacked RNN, we have multiple RNN layers. The output sequence of one layer serves as the input sequence for the layer directly above it.



**Mathematical Formulation:**
Let $h_t^{(l)}$ denote the hidden state of the $l$-th layer at time step $t$.
* **First Layer ($l=1$):** This layer operates on the input sequence $x_t$.
    $$
    h_t^{(1)} = f(W_{xh}^{(1)} x_t + W_{hh}^{(1)} h_{t-1}^{(1)} + b_h^{(1)})
    $$
* **Subsequent Layers ($l > 1$):** These layers operate on the hidden state sequence from the layer below, $h_t^{(l-1)}$.
    $$
    h_t^{(l)} = f(W_{xh}^{(l)} h_t^{(l-1)} + W_{hh}^{(l)} h_{t-1}^{(l)} + b_h^{(l)})
    $$
The final output of the model at time $t$, $\hat{y}_t$, is typically generated from the hidden state of the top-most layer, $L$.
$$
\hat{y}_t = g(W_y h_t^{(L)} + b_y)
$$

**Combining Architectures:**
These two extensions are not mutually exclusive. In fact, one of the most powerful and widely used RNN architectures in modern NLP is the **Deep Bidirectional RNN**. In this model, each layer in the stack is a Bi-RNN. The output of layer $l$ that gets passed to layer $l+1$ is the sequence of concatenated forward and backward hidden states from layer $l$. This allows the model to learn a deep hierarchy of features, with each level of the hierarchy having access to the full past and future context.