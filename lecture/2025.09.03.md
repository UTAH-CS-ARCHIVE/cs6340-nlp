### **[Lecture Note] Information Theory and Language Model Evaluation**

> Written by Hongseo Jang

### Kullback-Leibler (KL) Divergence

KL Divergence, also known as relative entropy, originates from information theory. It measures how one probability distribution $P$ is different from a second, reference probability distribution $Q$.

* **Intuitive Meaning:** KL Divergence can be thought of as a measure of "surprise" or "information loss." It quantifies the number of extra bits of information needed to encode samples from a true distribution $P$ when using a code that is optimized for an approximate distribution $Q$. If $P$ and $Q$ are identical, no information is lost, and the divergence is 0. The more $Q$ differs from $P$, the larger the information loss and the higher the KL divergence.

* **Mathematical Definition:** For two discrete probability distributions $P$ and $Q$ over the same sample space $\mathcal{X}$, the KL Divergence from $Q$ to $P$ is defined as:
    $$
    D_{KL}(P||Q) = \sum_{x \in \mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)
    $$
    This can also be written as the expectation of the logarithmic difference between the probabilities, with respect to the distribution $P$:
    $$
    D_{KL}(P||Q) = E_{x \sim P}\left[\log\frac{P(x)}{Q(x)}\right] = E_{x \sim P}[\log P(x) - \log Q(x)]
    $$
    *Note:* We must have $Q(x) > 0$ whenever $P(x) > 0$. If $P(x) > 0$ and $Q(x) = 0$, the divergence is infinite, indicating that $Q$ is a very poor approximation of $P$ because it assigns zero probability to an event that can actually occur.

<br>

#### The Asymmetry of KL Divergence

A critical property of KL Divergence is that it is **not a true mathematical distance metric**. This is because it is **asymmetric**, meaning that in general:
$$
D_{KL}(P||Q) \neq D_{KL}(Q||P)
$$

* **Mathematical Proof (by counterexample):**
    Let's consider a simple binary event space and two distributions:
    * $P = (0.9, 0.1)$
    * $Q = (0.5, 0.5)$
    Let's use the natural logarithm (ln) for our calculation.

    1.  **Calculate $D_{KL}(P||Q)$:**
        $$
        D_{KL}(P||Q) = P(1)\ln\left(\frac{P(1)}{Q(1)}\right) + P(2)\ln\left(\frac{P(2)}{Q(2)}\right)
        $$
        $$
        = 0.9 \ln\left(\frac{0.9}{0.5}\right) + 0.1 \ln\left(\frac{0.1}{0.5}\right)
        $$
        $$
        \approx 0.9 \times (0.5878) + 0.1 \times (-1.6094) \approx 0.529 - 0.161 = 0.368
        $$

    2.  **Calculate $D_{KL}(Q||P)$:**
        $$
        D_{KL}(Q||P) = Q(1)\ln\left(\frac{Q(1)}{P(1)}\right) + Q(2)\ln\left(\frac{Q(2)}{P(2)}\right)
        $$
        $$
        = 0.5 \ln\left(\frac{0.5}{0.9}\right) + 0.5 \ln\left(\frac{0.5}{0.1}\right)
        $$
        $$
        \approx 0.5 \times (-0.5878) + 0.5 \times (1.6094) \approx -0.294 + 0.805 = 0.511
        $$

    Since $0.368 \neq 0.511$, we have demonstrated that $D_{KL}(P||Q) \neq D_{KL}(Q||P)$.

* **Interpretive Difference:**
    * **$D_{KL}(P||Q)$:** Here, $P$ is the "true" distribution. The divergence gets very large if we have a high $P(x)$ but a low $Q(x)$. This penalizes $Q$ heavily for failing to predict likely events. This is often used in scenarios where we want our model ($Q$) to cover all the modes of the true data ($P$).
    * **$D_{KL}(Q||P)$:** Here, $Q$ is the reference. The divergence gets large if we have a high $Q(x)$ but a low $P(x)$. This penalizes $Q$ for predicting events that are actually unlikely. This forces $Q$ to be "conservative" and only place probability mass where $P$ has it.

<br>

### Jensen-Shannon Divergence (JSD)

The asymmetry of KL Divergence can be problematic when we need a true, symmetric measure of similarity. The Jensen-Shannon Divergence solves this problem by creating a symmetric version of KL Divergence.

* **Derivation:**
    The core idea is to avoid comparing $P$ and $Q$ directly. Instead, we first define an average or mixture distribution, $M$:
    $$
    M = \frac{1}{2}(P+Q)
    $$
    Then, we calculate the JSD as the average of the two (asymmetric) KL Divergences from $P$ and $Q$ to this mixture distribution $M$.

* **Mathematical Definition:**
    $$
    JSD(P||Q) = \frac{1}{2} D_{KL}(P||M) + \frac{1}{2} D_{KL}(Q||M)
    $$

* **Properties:**
    * **Symmetric:** It is immediately clear that $JSD(P||Q) = JSD(Q||P)$ because $M$ is defined symmetrically.
    * **Non-negative:** $JSD(P||Q) \ge 0$.
    * **Finite:** Unlike KL Divergence, JSD is always finite.
    * The square root of JSD, $\sqrt{JSD(P||Q)}$, is a valid distance metric, meaning it satisfies the triangle inequality.

<br>

### Applications in NLP

* **Evaluating Language Models:** We can compare the word distribution predicted by our language model ($Q$) with the actual distribution of words in a test set ($P$) to measure how well the model captures the true language statistics.
* **Topic Modeling:** JSD can be used to measure the similarity between the topic distributions of two different documents. This is useful for document clustering and similarity search.
* **Comparing Word Embeddings:** We can analyze the distributional properties of word embeddings by comparing the probability distributions of context words for two different target words.
* **Generative Models:** In training Generative Adversarial Networks (GANs) for text generation, JSD (or a variation) is minimized to encourage the distribution of the generated text to become closer to the distribution of the real text data.