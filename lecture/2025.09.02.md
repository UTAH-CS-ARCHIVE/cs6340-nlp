### **[Lecture Note] Information Theory and Language Model Evaluation**

> Written by Hongseo Jang

#### **1. Introduction to Information Theory**

Information theory is a branch of mathematics that deals with the quantification, storage, and communication of information. In Natural Language Processing, it provides a powerful mathematical framework for understanding and measuring uncertainty in language. This allows us to move beyond subjective evaluations of language models ("does this sentence sound good?") to objective, quantitative metrics. By using concepts from information theory, we can precisely measure how well a language model captures the statistical properties of a language.

<br>

#### **2. Entropy: Measuring Uncertainty**

**Entropy** is a fundamental concept in information theory that measures the average level of "information," "surprise," or "uncertainty" inherent in a random variable's possible outcomes. Let's consider a discrete random variable *X* with a set of possible outcomes *x* and a probability mass function *p(x)*. The entropy *H(p)* of this variable is defined as the expected value of the self-information of the variable.

The formula for entropy is:

$$
H(p) = -\sum_{x} p(x) \log_2 p(x)
$$

-   **p(x):** The probability of the event *x*.
-   **$\log_2 p(x)$:** The self-information of the outcome *x*. Outcomes with low probability have high self-information (they are more "surprising"), while high-probability outcomes have low self-information.
-   The negative sign ensures that the total entropy is non-negative, as the logarithm of a probability (a value between 0 and 1) is always non-positive.
-   The unit of entropy is "bits" when using a base-2 logarithm.

**Intuition:**
Imagine two coins. Coin A is a fair coin (Heads: 0.5, Tails: 0.5). Coin B is a biased coin (Heads: 0.9, Tails: 0.1). The outcome of flipping Coin A is more uncertain than flipping Coin B. Calculating the entropy confirms this: Coin A will have a higher entropy because its outcomes are more unpredictable.

In the context of language, entropy can be seen as a measure of the unpredictability of the next word. A language with very rigid grammatical rules and limited vocabulary would have low entropy, while a complex and flexible language would have higher entropy.

<br>

#### **3. Cross-Entropy: Measuring the Difference between Distributions**

While entropy measures the uncertainty of a single (true) probability distribution, **Cross-Entropy** measures the average number of bits needed to encode an event from a true distribution *p* using a coding scheme optimized for an estimated distribution *q*. It is a measure of the "distance" between two probability distributions.

The formula for cross-entropy is:

$$
H(p, q) = -\sum_{x} p(x) \log_2 q(x)
$$

-   **p(x):** The true probability of event *x* (from the true distribution *p*).
-   **q(x):** The probability of event *x* as predicted by our model (from the estimated distribution *q*).

In NLP, *p* represents the true probability distribution of words in a language (which we can approximate using a large test corpus), and *q* represents the probability distribution predicted by our language model. The cross-entropy, therefore, measures how well our model's predictions match the true distribution of words.

A key property of cross-entropy is that $H(p, q) \geq H(p)$. The cross-entropy will be lowest and equal to the true entropy only when the model's distribution *q* is identical to the true distribution *p*. Therefore, in machine learning, our goal is to train a model that minimizes the cross-entropy between its predicted distribution and the true data distribution. A lower cross-entropy value signifies a better model.

<br>

#### **4. Perplexity: Evaluating Language Models**

**Perplexity (PPL)** is the standard evaluation metric for language models. It is derived directly from cross-entropy and provides a more intuitive way to interpret a model's performance.

Perplexity is defined as the exponentiation of the cross-entropy:

$$
\text{Perplexity}(p, q) = 2^{H(p, q)}
$$

Substituting the cross-entropy formula for a language model over a sequence of *N* words:

$$
H(p, q) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 q(w_i | w_1, \dots, w_{i-1})
$$

The perplexity is:

$$
\text{PPL} = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 q(w_i | w_1, \dots, w_{i-1})} = \left( \prod_{i=1}^{N} q(w_i | w_1, \dots, w_{i-1}) \right)^{-\frac{1}{N}}
$$

**Intuition:**
Perplexity can be understood as the **weighted average branching factor** of a language model. A perplexity of *k* means that, on average, the model is as confused as if it had to choose uniformly and independently from *k* options at each time step.

-   **Lower is better:** A lower perplexity indicates that the model is less "perplexed" by the test set and makes better predictions. The probability distribution of the model is closer to the true distribution of the language.
-   **Ideal PPL:** An ideal language model that perfectly predicts the next word every time would assign a probability of 1 to the correct word and 0 to all others, resulting in a perplexity of 1.
-   **Random PPL:** A model that assigns a uniform probability of $1/|V|$ to each of the $|V|$ words in the vocabulary would have a perplexity of $|V|$.

By using perplexity, we can compare different language models on a standardized test set to determine which one has a better statistical understanding of the language.