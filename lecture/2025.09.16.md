### **[Lecture Note] Vector Representations of Words (Word Embeddings)**

> Written by Hongseo Jang

#### **1. The Idea of Dense Vector Representations**

In traditional NLP methods like N-gram models or Naive Bayes, words are often treated as discrete, atomic symbols. This can be represented using **one-hot vectors**, where each word is a vector with a dimension equal to the vocabulary size, consisting of all zeros except for a single '1' at the index corresponding to that word.

However, this approach has two major drawbacks:
1.  **High Dimensionality and Sparsity:** For a typical vocabulary of 50,000 words, each word vector would have 50,000 dimensions, making it computationally expensive and memory-intensive. These vectors are extremely sparse (mostly zeros).
2.  **No Notion of Similarity:** One-hot vectors are orthogonal to each other. The dot product between any two distinct word vectors is zero, meaning this representation carries no inherent information about semantic similarity. For example, the vectors for "cat" and "kitten" are just as distant as the vectors for "cat" and "airplane".

**Word embeddings** solve these issues by representing words as low-dimensional, **dense vectors** (typically 50 to 300 dimensions). In this continuous vector space, semantically similar words are located closer to each other. This allows models to generalize better, as they can understand that, for instance, a sentence about a "kitten" is semantically close to a sentence about a "cat".

<br>

#### **2. The Distributional Hypothesis**

The underlying principle that makes word embeddings possible is the **Distributional Hypothesis**, famously summarized by linguist J.R. Firth in 1957 as: "You shall know a word by the company it keeps."

This hypothesis states that words that appear in similar contexts tend to have similar meanings. For example, words like "coffee," "tea," and "juice" are likely to appear in contexts such as "I drank a cup of ___." or "He likes ___ with his breakfast." By analyzing the statistical patterns of word co-occurrences in a large corpus of text, we can learn a vector representation for each word that captures its meaning.

<br>

#### **3. Matrix Factorization Methods**

One way to capture co-occurrence statistics is by constructing a large matrix from a corpus and then factorizing it to obtain dense vectors.

**a) Pointwise Mutual Information (PMI)**

First, we build a word-word co-occurrence matrix, where each cell counts how often two words appear together in a predefined context window. However, raw counts can be misleading as they are dominated by very frequent words. A more informative measure of association is **Pointwise Mutual Information (PMI)**.

PMI between two words, $w_1$ and $w_2$, measures how much more likely they are to co-occur than if they were independent. It is defined as:

$$
\text{PMI}(w_1, w_2) = \log_2 \frac{P(w_1, w_2)}{P(w_1)P(w_2)}
$$

-   $P(w_1, w_2)$: The probability of observing both words together. This can be estimated as $\frac{\text{Count}(w_1, w_2)}{\text{Total number of co-occurrence pairs}}$.
-   $P(w_1)$ and $P(w_2)$: The individual probabilities of observing each word.
-   **Interpretation:**
    -   $\text{PMI} > 0$: The words co-occur more often than by chance (high association).
    -   $\text{PMI} \approx 0$: The words co-occur about as often as expected by chance.
    -   $\text{PMI} < 0$: The words co-occur less often than by chance.

In practice, we often use **Positive PMI (PPMI)**, where negative PMI values are replaced with 0, as negative association is often noisy and less reliable. We can construct a large PPMI matrix where each cell contains the PPMI value for a pair of words.

**b) Singular Value Decomposition (SVD)**

The PPMI matrix is high-dimensional and sparse. To obtain low-dimensional, dense word vectors, we can apply a dimensionality reduction technique like **Singular Value Decomposition (SVD)**.

SVD is a fundamental theorem of linear algebra which states that any matrix *X* can be decomposed into the product of three other matrices:

$$
X = U \Sigma V^T
$$

-   *X*: The original $m \times n$ matrix (e.g., our PPMI matrix).
-   *U*: An $m \times m$ orthogonal matrix (left singular vectors).
-   $\Sigma$: An $m \times n$ diagonal matrix with non-negative real numbers on the diagonal, known as singular values.
-   $V^T$: The transpose of an $n \times n$ orthogonal matrix (right singular vectors).

By applying SVD to our PPMI matrix, we can approximate the original matrix by keeping only the top *k* singular values, effectively reducing the dimensionality. The dense word embedding for a word is then typically taken from the first *k* columns of the *U* matrix. These new *k*-dimensional vectors capture the most significant latent semantic relationships present in the original co-occurrence data.

<br>

#### **4. Word2Vec: A Prediction-Based Approach**

Instead of counting co-occurrences and then performing matrix factorization, **Word2Vec** is a predictive model that learns embeddings directly. It's a shallow neural network that is trained to predict a word from its context (or vice versa). The learned weights of the network then become the word embeddings. There are two main architectures:

1.  **Continuous Bag-of-Words (CBOW):** The model takes the surrounding context words as input (e.g., ["the", "cat", "on", "the"]) and is trained to predict the target word in the middle ("sat"). The input vectors are averaged before being passed to the hidden layer.
2.  **Skip-gram:** This architecture works in the opposite direction. It takes a single target word as input (e.g., "sat") and is trained to predict the surrounding context words (e.g., "the", "cat", "on", "the"). It tends to perform better for infrequent words and is the more popular of the two methods.

The mathematical core of Word2Vec is to learn an embedding matrix *W* (where each row is a word vector) and a context matrix *C*. During training for the Skip-gram model, for a given target word *t* and a context word *c*, the model calculates a score using the dot product of their vectors ($v_t \cdot v_c$). This score is then passed through a softmax function to produce a probability. The model's weights (the vectors in *W* and *C*) are adjusted via backpropagation to maximize the probability of true context words appearing. This process forces the vectors of words that appear in similar contexts to become similar to each other.