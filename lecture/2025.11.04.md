### **[Lecture Note] The Attention Mechanism**

> Written by Hongseo Jang

#### **1. The Limitations of Sequence-to-Sequence (Seq2Seq) Models**

The standard Sequence-to-Sequence (Seq2Seq) model, also known as the Encoder-Decoder model, consists of two main components:
1.  An **Encoder RNN** that reads the entire input sequence (e.g., a sentence in French) and compresses it into a single, fixed-length vector called the **context vector**.
2.  A **Decoder RNN** that takes this context vector and generates the output sequence (e.g., the translated sentence in English).

The primary limitation of this architecture is the **information bottleneck**. The single context vector must encode the entire meaning of the input sequence. For long and complex sentences, this is an incredibly difficult task, and the model often loses crucial information. The decoder's performance is heavily dependent on the quality of this single vector, and its ability to remember information from the beginning of a long input sequence is poor.

<br>

#### **2. The Core Idea of Attention**

The Attention mechanism was introduced to overcome this limitation. The core idea is to allow the decoder to look back at the **entire sequence of the encoder's hidden states** at every step of the output generation.

Instead of forcing the encoder to create one single context vector, the attention mechanism creates a unique context vector for each decoding time step. This context vector is a **weighted sum** of all the encoder hidden states. The weights are calculated at each decoding step and determine which parts of the input sequence are most relevant for generating the current output word.

This is analogous to human attention. When translating a sentence, a person doesn't just read the entire sentence once and then start translating from memory. Instead, they focus on specific, relevant words in the source sentence as they generate each word of the translation. The attention mechanism provides a way for neural networks to learn this "focusing" behavior.

<br>

#### **3. The Attention Process: Query, Key, and Value**

The attention mechanism can be conceptually broken down into an interaction between three components: a **Query**, a set of **Keys**, and a set of **Values**.

-   **Query (Q):** The Query represents the current state of the decoder. It is typically the decoder's hidden state from the previous time step ($h_{t-1}^{dec}$). The Query is essentially asking a question: "Given my current state, which part of the input sequence is the most important for me to focus on to generate the next word?"

-   **Keys (K):** The Keys are a set of vectors corresponding to all the encoder's hidden states ($h_1^{enc}, h_2^{enc}, \dots, h_N^{enc}$). Each Key vector acts as a "label" or "identifier" for a specific piece of information in the input sequence.

-   **Values (V):** The Values are also a set of vectors. In the context of the original Seq2Seq attention, the Value vectors are often the same as the Key vectors (i.e., the encoder hidden states themselves). The Values contain the actual information that we want to combine to form the context vector.

The process of generating a context vector using these components involves three steps:

1.  **Calculate Similarity Scores:** The Query vector is compared with every Key vector. This comparison yields a score that quantifies the relevance or similarity between the decoder's current state and each of the encoder's states. Common similarity measures include the **dot product** and **cosine similarity**.

2.  **Compute Attention Weights:** The raw similarity scores are passed through a **softmax function**. This normalizes the scores, converting them into a set of positive weights that sum to 1. These weights form a probability distribution over the input sequence, representing the "attention" that the decoder should pay to each input word.

3.  **Generate the Context Vector:** The final context vector is computed as a **weighted sum** of all the Value vectors, using the attention weights calculated in the previous step. This ensures that the Values (encoder states) that are most relevant to the Query (current decoder state) contribute more heavily to the final context vector.

<br>

#### **4. Scaled Dot-Product Attention**

A highly efficient and widely used implementation of this process is the **Scaled Dot-Product Attention**, which is a cornerstone of the Transformer architecture. It performs the entire attention calculation using optimized matrix operations.

The formula is given as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Let's break down this mathematical expression:

1.  $QK^T$: This is the first step, calculating the similarity scores. The matrix multiplication of the Query matrix *Q* and the transpose of the Key matrix *K* efficiently computes the dot product between every query vector and every key vector. The result is a matrix of raw attention scores.

2.  $\frac{1}{\sqrt{d_k}}$: This is the **scaling factor**. $d_k$ is the dimension of the key vectors. For high-dimensional key vectors, the dot products can grow very large in magnitude. Large inputs to the softmax function can cause its gradients to become vanishingly small, making training difficult. Dividing the scores by the square root of the dimension helps to moderate the size of these scores, leading to more stable gradients.

3.  $\text{softmax}(\dots)$: The softmax function is applied to the scaled scores. This converts the scores into the final attention weights, ensuring they are all positive and sum to 1.

4.  $(\dots)V$: The resulting matrix of attention weights is then multiplied by the Value matrix *V*. This final matrix multiplication performs the weighted sum of the value vectors, producing the output of the attention layer. This output is the context vector (or a set of context vectors if there are multiple queries), which contains rich, contextually relevant information from the input sequence, ready to be used by the next part of the model.