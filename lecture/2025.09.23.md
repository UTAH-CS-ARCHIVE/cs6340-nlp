### **[Lecture Note] Optimization of Word2Vec**

> Written by Hongseo Jang

#### **1. The Skip-gram Objective Function**

The goal of the Word2Vec Skip-gram model is to learn word vectors that are good at predicting the context words given a center word. To formalize this, we need to define an objective function (also known as a loss function) that quantifies how well the model is performing. Our aim is to adjust the model's parameters—the word vectors themselves—to maximize this objective.

Let's consider a center word *c* and a context word *o*. The probability of observing the context word *o* given the center word *c*, parameterized by $\theta$ (which represents all the word vectors), is defined using the **softmax function**:

$$
P(o|c; \theta) = \frac{\exp(v_o^T v_c)}{\sum_{w \in V} \exp(v_w^T v_c)}
$$

Here, $v_o$ and $v_c$ are the vector representations for the context and center words, respectively, and *V* is the entire vocabulary. The dot product $v_o^T v_c$ measures the similarity between the two word vectors. The softmax function normalizes these similarity scores into a valid probability distribution over all possible words in the vocabulary.

The goal is to maximize the joint probability of observing the true context words for every word in the corpus. This is equivalent to minimizing the average negative log-likelihood. This gives us the Word2Vec objective function, $J(\theta)$:

$$
J(\theta) = - \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \le j \le c, j \ne 0} \log P(w_{t+j} | w_t; \theta)
$$

-   *T* is the total number of words in the training corpus.
-   The outer sum iterates through each word $w_t$ in the corpus.
-   The inner sum iterates through the context words $w_{t+j}$ within a window of size *c* around the center word $w_t$.
-   Minimizing this function means we are adjusting the vectors $\theta$ to make the probabilities of the true context words as high as possible.

<br>

#### **2. Optimization via Gradient Descent**

To minimize the loss function $J(\theta)$, we use an optimization algorithm called **Gradient Descent**. The core idea is to iteratively update the parameters ($\theta$) in the direction opposite to the gradient of the loss function. The gradient, $\nabla_{\theta} J(\theta)$, tells us the direction of the steepest ascent of the loss function, so moving in the opposite direction leads us towards a minimum.

The update rule for a parameter $\theta_i$ is:
$$
\theta_i := \theta_i - \eta \nabla_{\theta_i} J(\theta)
$$
where $\eta$ is the learning rate, a small hyperparameter that controls the step size.

However, calculating the gradient for the entire corpus *T* at each step (Batch Gradient Descent) is computationally infeasible. The vocabulary *V* can be very large (e.g., >100,000), making the summation in the denominator of the softmax extremely expensive to compute for every single update.

To solve this, we use **Stochastic Gradient Descent (SGD)**. Instead of computing the loss and gradient over the entire dataset, SGD approximates them by considering just a single training sample at a time (or a small mini-batch). In the context of Skip-gram, a single sample would be a (center word, context word) pair. For each sample, we compute the gradient and update the parameters. While each update is "noisy," the process converges much faster and is far more computationally efficient.

<br>

#### **3. The Computational Bottleneck and Negative Sampling**

Even with SGD, the softmax calculation remains a major bottleneck due to the normalization term $\sum_{w \in V} \exp(v_w^T v_c)$. We would still need to compute a dot product for the center word vector with every other word vector in the vocabulary just to update for a single training pair.

To overcome this, Word2Vec introduces several optimization techniques, the most popular being **Negative Sampling**.

**The Idea:**
Instead of training a large multiclass classifier (the softmax), Negative Sampling transforms the problem into a series of binary classification tasks. For a given pair of a center word *c* and a true context word *o* (a positive sample), the goal is to train a model that can distinguish this true pair from several randomly chosen "negative" pairs, which consist of the center word *c* and a word *k* that is *not* in its context.

**The Mathematics:**
We replace the softmax function with a logistic (sigmoid) function, $\sigma(x) = 1/(1+e^{-x})$, which outputs the probability of a pair being a true context pair. The new objective for a single positive pair `(c, o)` is to:
1.  Maximize the probability that `(c, o)` is a true pair: $\sigma(v_o^T v_c)$.
2.  Minimize the probability that `(c, k)` is a true pair for *K* randomly sampled negative words, *k*: $\sigma(v_k^T v_c)$. This is equivalent to maximizing the probability that they are *not* a true pair: $\sigma(-v_k^T v_c)$.

This leads to the following objective function for the pair `(c, o)`, which we want to maximize:

$$
L(\theta) = \log \sigma(v_o^T v_c) + \sum_{i=1}^{K} \mathbb{E}_{k_i \sim P_n(w)}[\log \sigma(-v_{k_i}^T v_c)]
$$

-   The first term encourages the vectors of the true pair to be similar.
-   The second term encourages the vectors of the negative pairs to be dissimilar.
-   *K* is the number of negative samples (typically 5-20).
-   $P_n(w)$ is the noise distribution from which negative samples are drawn.

Crucially, with Negative Sampling, we only need to compute and update the vectors for the true context word and the *K* negative samples, rather than all $|V|$ words in the vocabulary.

The noise distribution $P_n(w)$ is typically based on the unigram frequency of words in the corpus, raised to the 3/4 power: $P_n(w) \propto U(w)^{3/4}$. This heuristic gives slightly more probability to less frequent words, preventing common words like "the" or "a" from dominating the negative samples.