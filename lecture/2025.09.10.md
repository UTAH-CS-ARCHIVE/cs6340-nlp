### **[Lecture Note] Text Classification and the Naive Bayes Classifier**

> Written by Hongseo Jang

### From Linear Regression to Logistic Regression

To understand Logistic Regression, it's best to start with its simpler cousin, Linear Regression.

1.  **Linear Regression Recap:** The goal of linear regression is to predict a continuous target value, $y$. It assumes a linear relationship between the input features $x$ and the output. The model is simply a weighted sum of the features:
    $$
    y_{pred} = w_0 + w_1x_1 + \dots + w_nx_n = w^T x
    $$
    The output of this function is a real number ranging from $(-\infty, \infty)$.

2.  **The Problem for Classification:** In binary classification, our target $y$ is categorical (e.g., 0 or 1). We want our model to output a probability, which must be in the range $[0, 1]$. The unbounded output of a linear model is not suitable for this.

3.  **The Bridge: Log-Odds (Logit):** How do we map the $(-\infty, \infty)$ range to $[0, 1]$? We can use the concept of odds and log-odds.
    * Let $p$ be the probability of the positive class, i.e., $p = P(y=1 | x)$.
    * The **odds** of the event happening are defined as the ratio of the probability of it happening to it not happening:
        $$
        \text{Odds} = \frac{p}{1-p}
        $$
        The range of the odds is $[0, \infty)$. This is closer, but still not the $(-\infty, \infty)$ we need.
    * By taking the natural logarithm of the odds, we get the **log-odds** or **logit** function:
        $$
        \text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
        $$
        The range of the logit function is $(-\infty, \infty)$, which perfectly matches the output range of our linear model.

4.  **The Logistic Regression Model:** The core assumption of Logistic Regression is that the log-odds are a linear function of the input features $x$.
    $$
    \ln\left(\frac{p}{1-p}\right) = w^T x
    $$

<br>

### The Sigmoid (Logistic) Function

To get back to the probability $p$, we need to solve the equation above for $p$. This inverse of the logit function is the **sigmoid** or **logistic** function.

* **Derivation:**
    $$
    \frac{p}{1-p} = e^{w^T x}
    $$
    $$
    p = (1-p)e^{w^T x} \implies p = e^{w^T x} - p \cdot e^{w^T x}
    $$
    $$
    p(1 + e^{w^T x}) = e^{w^T x}
    $$
    $$
    p = \frac{e^{w^T x}}{1 + e^{w^T x}}
    $$
    By dividing the numerator and denominator by $e^{w^T x}$, we get the standard form. Let $z = w^T x$:
    $$
    p = \frac{1}{e^{-z} + 1} = \frac{1}{1 + e^{-z}} = \sigma(z)
    $$
* **Role and Properties:**
    * The sigmoid function, $\sigma(z)$, takes any real-valued number $z$ and "squashes" it into the range $(0, 1)$.
    * This allows us to interpret the model's output, $\sigma(w^T x)$, as the estimated probability that the input $x$ belongs to the positive class ($y=1$).
    * It has a convenient derivative, $\sigma'(z) = \sigma(z)(1 - \sigma(z))$, which simplifies the gradient calculations needed for training the model.

<br>

### The Loss Function: Binary Cross-Entropy

To train the model (i.e., find the best weights $w$), we need a loss function that measures how "wrong" the model's predictions are. While Mean Squared Error (MSE) is used for Linear Regression, it creates a non-convex loss landscape for Logistic Regression, making optimization difficult. Instead, we derive a more suitable loss function from the principle of **Maximum Likelihood Estimation (MLE)**.

* **The Likelihood of a Single Data Point:**
    Our model predicts $\hat{y} = P(y=1|x) = \sigma(w^T x)$. The probability of the other class is $P(y=0|x) = 1 - \sigma(w^T x) = 1 - \hat{y}$. We can write a single expression for the probability of observing the true label $y \in \{0, 1\}$ using the Bernoulli distribution:
    $$
    P(y|\hat{y}) = (\hat{y})^y (1-\hat{y})^{1-y}
    $$
    If the true label $y=1$, this simplifies to $\hat{y}$. If $y=0$, it simplifies to $1-\hat{y}$.

* **The Log-Likelihood of the Dataset:**
    Assuming all our $m$ data points are independent and identically distributed (i.i.d.), the total likelihood of observing our entire dataset is the product of the individual probabilities. To make the math easier, we work with the log-likelihood:
    $$
    \ell(w) = \log \left( \prod_{i=1}^{m} (\hat{y}_i)^{y_i} (1-\hat{y}_i)^{1-y_i} \right) = \sum_{i=1}^{m} \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]
    $$

* **From Likelihood Maximization to Loss Minimization:**
    The goal of MLE is to find the weights $w$ that maximize this log-likelihood function. In machine learning, we typically frame this as minimizing a **loss function**. The standard choice is the **negative log-likelihood**. For a single training example, the loss is:
    $$
    J(w) = - \left[ y \log(\hat{y}) + (1-y) \log(1-\hat{y}) \right]
    $$
    This is the **Binary Cross-Entropy** or **Log Loss** function.

* **Why it's a good loss function:**
    * **Convexity:** This loss function is convex, which guarantees that an optimization algorithm like gradient descent can find the single global minimum.
    * **Penalizes Wrong Predictions:** It heavily penalizes confident but incorrect predictions. For instance, if the true label is $y=1$ but the model confidently predicts $\hat{y} \to 0$, the term $y \log(\hat{y})$ goes to $-\infty$, and the loss goes to $+\infty$. This provides a strong gradient signal for the model to correct its weights.