### **[Lecture Note] Text Classification and the Naive Bayes Classifier**

> Written by Hongseo Jang

#### **1. Introduction to Text Classification**

Text classification is a fundamental task in NLP that involves assigning a predefined category or label to a piece of text. It is a supervised machine learning problem where a model is trained on a dataset of texts that have been manually labeled with their respective classes. Once trained, the model can then be used to classify new, unseen texts.

Common applications of text classification include:
-   **Spam Mail Filtering:** Classifying emails into "spam" or "not spam" (ham).
-   **Sentiment Analysis:** Determining the sentiment of a product review or a social media post (e.g., "positive," "negative," "neutral").
-   **Topic Labeling:** Assigning news articles to categories like "sports," "politics," or "technology."
-   **Language Detection:** Identifying the language in which a text is written.

<br>

#### **2. The Principle of Naive Bayes**

The Naive Bayes classifier is a probabilistic classifier based on **Bayes' Theorem**. It is particularly well-suited for text classification due to its simplicity and efficiency.

The core task is to find the most likely class *y* (e.g., "spam") for a given document *x*. In probabilistic terms, we want to find the class *y* that maximizes the posterior probability $P(y|x)$.

$$
\hat{y} = \underset{y}{\arg\max} \, P(y|x)
$$

Calculating $P(y|x)$ directly is difficult. However, we can use Bayes' Theorem to re-express it in terms of quantities that are easier to estimate from the data:

$$
P(y|x) = \frac{P(x|y)P(y)}{P(x)}
$$

Let's break down the terms:
-   $P(y|x)$: The **posterior probability**. The probability of class *y* given the document *x*. This is what we want to compute.
-   $P(x|y)$: The **likelihood**. The probability of observing the document *x* given that it belongs to class *y*.
-   $P(y)$: The **prior probability**. The overall probability of class *y*, irrespective of any data.
-   $P(x)$: The **evidence**. The probability of observing the document *x*.

When we are trying to find the most likely class for a given document *x*, the value of $P(x)$ is the same for all classes. Therefore, we can ignore it for the purpose of maximization:

$$
\hat{y} = \underset{y}{\arg\max} \, P(x|y)P(y)
$$

This is the central equation for the Naive Bayes classifier. We choose the class that maximizes the product of the likelihood and the prior.

<br>

#### **3. The "Naive" Assumption of Conditional Independence**

To make the calculation of the likelihood $P(x|y)$ feasible, we represent the document *x* as a collection of its words (features), so $x = (x_1, x_2, \dots, x_n)$, where $x_i$ is the i-th word in the document.

The likelihood term then becomes $P(x_1, x_2, \dots, x_n | y)$. Estimating this joint probability is extremely difficult due to the vast number of possible word combinations (the curse of dimensionality).

This is where the **"naive" assumption** comes in. The Naive Bayes classifier makes a strong, simplifying assumption that all features (words) are **conditionally independent** of each other, given the class *y*. This means we assume that the presence of one word in a document does not affect the presence of another, once we know the document's class.

Mathematically, this assumption is expressed as:

$$
P(x_1, x_2, \dots, x_n | y) = P(x_1|y) \times P(x_2|y) \times \dots \times P(x_n|y) = \prod_{i=1}^{n} P(x_i|y)
$$

While this assumption is rarely true in real language (e.g., the word "York" is much more likely to appear if the word "New" is already present), the Naive Bayes classifier often performs surprisingly well in practice.

By substituting this into our maximization formula, we get the final decision rule for the Naive Bayes classifier:

$$
\hat{y} = \underset{y}{\arg\max} \, P(y) \prod_{i=1}^{n} P(x_i|y)
$$

In practice, to avoid numerical underflow from multiplying many small probabilities, we often work with the sum of log-probabilities:

$$
\hat{y} = \underset{y}{\arg\max} \, \left( \log P(y) + \sum_{i=1}^{n} \log P(x_i|y) \right)
$$

<br>

#### **4. Data Sparsity and Laplace Smoothing**

To use the Naive Bayes rule, we need to estimate the prior $P(y)$ and the likelihoods $P(x_i|y)$ from our training data. Using Maximum Likelihood Estimation (MLE), these are simply frequency counts.

However, a significant problem arises: **data sparsity**. What if a word in our test document, say "win," never appeared in a "ham" (not spam) document in our training set? According to MLE, the probability $P(\text{"win"}|\text{ham})$ would be 0.

This single zero probability would cause the entire product for the "ham" class to become zero, meaning the document could never be classified as "ham," regardless of how many other strong "ham" indicators it contains.

To solve this, we use a technique called **Laplace Smoothing** (or add-one smoothing). The idea is to pretend we have seen every word in our vocabulary at least one more time than we actually did. We add a smoothing parameter $\alpha$ (typically $\alpha=1$) to the numerator and adjust the denominator accordingly.

Let $|V|$ be the size of the vocabulary. The smoothed probability estimate is:

$$
P_{\text{Laplace}}(x_i|y) = \frac{\text{Count}(x_i, y) + \alpha}{\left(\sum_{w \in V} \text{Count}(w, y)\right) + \alpha|V|}
$$

Here, $\text{Count}(x_i, y)$ is the number of times word $x_i$ appears in documents of class *y*, and the denominator term is the total number of words in all documents of class *y*. By adding $\alpha$ to the numerator, we ensure that no word has a zero probability. Adding $\alpha|V|$ to the denominator ensures that the probabilities still sum to 1. This makes the classifier more robust to unseen words.