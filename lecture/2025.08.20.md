### **[Lecture Note] The Role of Mathematics in Natural Language Processing**

> Written by Hongseo Jang

### Core Python Libraries: Numpy & Scipy

**1. Numpy (Numerical Python)**
* **What it is:** The fundamental package for scientific computing in Python. Its core feature is the `ndarray` (n-dimensional array) object.
* **Why it's important for NLP:** NLP data, once vectorized, is essentially a collection of numbers. Numpy provides highly efficient data structures and operations for handling these large numerical datasets. Standard Python lists are slow for element-wise mathematical operations, whereas Numpy arrays are stored in a contiguous block of memory, enabling vectorized operations that are executed in compiled C code, leading to significant performance gains.
* **Key Operations:**
    * Creating vectors (1D arrays) and matrices (2D arrays).
    * Performing element-wise operations (addition, multiplication).
    * Linear algebra operations: dot product, matrix multiplication, norm calculation.

    *Example in Python:*
    ```python
    import numpy as np

    # Representing words as vectors
    word_vec1 = np.array([0.1, 0.5, 0.3])
    word_vec2 = np.array([0.2, 0.4, 0.9])

    # Element-wise addition
    sum_vec = word_vec1 + word_vec2

    # Dot product
    dot_product = np.dot(word_vec1, word_vec2)
    ```

**2. Scipy (Scientific Python)**
* **What it is:** A library that builds upon Numpy and provides a large collection of algorithms and high-level functions for scientific and technical computing.
* **Why it's important for NLP:** While Numpy provides the basic building blocks (arrays and fundamental operations), Scipy offers more specialized functions. In NLP, this could include functions for sparse matrices (`scipy.sparse`), which are crucial when dealing with high-dimensional vocabulary spaces where most entries are zero (e.g., in a document-term matrix). It also provides advanced optimization and statistical functions.

<br>

### Linear Algebra Fundamentals for NLP

**1. Vector Space Model (VSM)**
The Vector Space Model is a conceptual model where we represent text documents (or any text unit) as vectors in a multi-dimensional space.
* **Core Idea:** Each dimension of the space corresponds to a unique term (word) in the vocabulary. The value in each dimension of a document's vector represents the importance, frequency, or weight of that term in the document (e.g., TF-IDF score).
* **Significance:** By mapping text to vectors, we can use mathematical tools to perform analysis. For instance, we can measure the "distance" or "angle" between two vectors to determine the similarity between the corresponding documents. Documents that are semantically similar are expected to be closer together in this vector space.

**2. Vector Norms**
A norm is a function that assigns a strictly positive length or size to each vector in a vector space. In NLP, it's often used to measure the magnitude of a vector representation of a word or document.

* **$L_2$ Norm (Euclidean Norm):**
    * This is the most common norm, representing the standard "straight-line" distance from the origin to the point defined by the vector.
    * For a vector $v = (v_1, v_2, \dots, v_n)$, the $L_2$ norm is calculated as:
        $$
        \|v\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}
        $$
    * **Geometric Interpretation:** It is the length of the vector in Euclidean space. In machine learning, $L_2$ regularization (Ridge) uses this to penalize large weights in a model, preventing overfitting by discouraging overly complex models.

* **$L_1$ Norm (Manhattan Norm or Taxicab Norm):**
    * This norm measures the sum of the absolute values of the vector's components.
    * For a vector $v = (v_1, v_2, \dots, v_n)$, the $L_1$ norm is:
        $$
        \|v\|_1 = \sum_{i=1}^{n} |v_i|
        $$
    * **Geometric Interpretation:** It represents the distance one would travel between two points in a grid-like path (like navigating city blocks). In machine learning, $L_1$ regularization (Lasso) often leads to sparse solutions, meaning many model weights become exactly zero. This is useful for feature selection.

**3. Dot Product (Inner Product)**
The dot product of two vectors is a fundamental operation that relates to the angle between them.
* **Algebraic Definition:** For two vectors $u = (u_1, \dots, u_n)$ and $v = (v_1, \dots, v_n)$:
    $$
    u \cdot v = \sum_{i=1}^{n} u_i v_i
    $$
* **Geometric Definition:**
    $$
    u \cdot v = \|u\|_2 \|v\|_2 \cos(\theta)
    $$
    where $\theta$ is the angle between the vectors $u$ and $v$.
* **Geometric Interpretation:** The dot product measures how much one vector "points" in the direction of another.
    * If $u \cdot v > 0$, the angle between them is acute ($< 90^{\circ}$).
    * If $u \cdot v = 0$, the vectors are orthogonal (perpendicular, $\theta = 90^{\circ}$).
    * If $u \cdot v < 0$, the angle is obtuse ($> 90^{\circ}$).

**4. Matrix Multiplication**
* **As an operation:** In NLP, a matrix can represent a collection of document vectors (a document-term matrix) or the weights of a neural network layer. Multiplying a vector by a matrix can be seen as transforming that vector from one space to another.
* **Geometric Interpretation:** Matrix multiplication is a **linear transformation**. It can rotate, scale, and shear vectors in the space. For example, in a simple neural network layer, input vectors (representing text) are multiplied by a weight matrix to be transformed into a new representation that is more useful for the downstream task.

<br>

### A Key Application: Cosine Similarity

One of the most direct applications of these linear algebra concepts in NLP is measuring the similarity between two documents.

* **Motivation:** We need a way to quantify how "similar" two documents are based on their vector representations. Using Euclidean distance ($L_2$ norm of the difference between vectors) can be misleading. A document that is twice as long as another but discusses the same topic might be far away in Euclidean space, even though it's semantically very similar.
* **Definition:** Cosine similarity measures the cosine of the angle between two vectors. It is derived directly from the geometric definition of the dot product and focuses purely on the orientation of the vectors, not their magnitude.
    $$
    \text{similarity}(u, v) = \cos(\theta) = \frac{u \cdot v}{\|u\|_2 \|v\|_2} = \frac{\sum_{i=1}^{n} u_i v_i}{\sqrt{\sum_{i=1}^{n} u_i^2} \sqrt{\sum_{i=1}^{n} v_i^2}}
    $$
* **Interpretation:**
    * The value ranges from -1 to 1.
    * **1:** The vectors point in the exact same direction (maximally similar).
    * **0:** The vectors are orthogonal, indicating no similarity (e.g., they share no common terms).
    * **-1:** The vectors point in opposite directions (maximally dissimilar).
    * In most NLP applications using term frequencies (like TF-IDF), vector components are non-negative, so cosine similarity typically ranges from 0 to 1.