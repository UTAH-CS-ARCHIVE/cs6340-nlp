# [Lecture Notes] Statistical Models in NLP

## 1. N-gram Models

### 1.1 Definition and Motivation
An **N-gram model** is a probabilistic language model based on the **Markov assumption** that the probability of a word depends only on the previous $n-1$ words.

Formally:

$$
P(w_1, w_2, \dots, w_T) \approx \prod_{t=1}^{T} P(w_t \mid w_{t-n+1}^{t-1})
$$

where $w_{t-n+1}^{t-1}$ denotes the $(n-1)$ preceding words.

### 1.2 Probability Estimation
For a bigram model ($n=2$):

$$
P(w_t \mid w_{t-1}) = \frac{\text{Count}(w_{t-1}, w_t)}{\text{Count}(w_{t-1})}
$$

### 1.3 Maximum Likelihood Estimation (MLE)
General formula:

$$
\hat{P}(w_t \mid w_{t-n+1}^{t-1}) = \frac{\text{Count}(w_{t-n+1}^{t})}{\text{Count}(w_{t-n+1}^{t-1})}
$$

### 1.4 Limitations
- **Data sparsity**: Many n-grams never appear in the training data.  
- **Smoothing needed**: Techniques like Laplace smoothing, Kneser-Ney.  
- **Long-range dependency problem**: Context window limited to $n-1$.  




## 2. Markov Chains

### 2.1 Markov Property
A stochastic process $\{X_t\}$ satisfies the **Markov property** if:

$$
P(X_t \mid X_{1}, X_{2}, \dots, X_{t-1}) = P(X_t \mid X_{t-1})
$$

### 2.2 Transition Probabilities
Let $P_{ij} = P(X_{t+1}=j \mid X_t=i)$ be the **transition probability matrix**.  
The sum of each row equals 1:

$$
\sum_j P_{ij} = 1
$$

### 2.3 Stationary Distribution
A distribution $\pi$ is **stationary** if:

$$
\pi = \pi P
$$
This means if the chain starts with $\pi$, it remains $\pi$ over time.

### 2.4 Example Derivation
Suppose we have two states {Rainy, Sunny} with transition matrix:

$$
P = \begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
$$

Stationary distribution $\pi = (\pi_R, \pi_S)$ satisfies:

$$
\pi_R = 0.7 \pi_R + 0.4 \pi_S \\
\pi_S = 0.3 \pi_R + 0.6 \pi_S \\
\pi_R + \pi_S = 1
$$

Solving gives $\pi_R = 0.57, \pi_S = 0.43$.



## 3. Hidden Markov Models (HMM)

### 3.1 Motivation
HMM extends Markov chains by introducing **hidden states** that generate **observations**.  
Example: Speech recognition â†’ hidden phonemes generate observed acoustic signals.

### 3.2 Components of HMM
- Hidden states: $Z_t \in \{1, \dots, K\}$  
- Observations: $X_t$  
- Parameters:
  - Initial distribution: $\pi_i = P(Z_1=i)$  
  - Transition matrix: $A_{ij} = P(Z_{t+1}=j \mid Z_t=i)$  
  - Emission distribution: $B_i(x) = P(X_t=x \mid Z_t=i)$  

### 3.3 Joint Probability
For a sequence $Z_{1:T}, X_{1:T}$:

$$
P(Z_{1:T}, X_{1:T}) = \pi_{Z_1} \, B_{Z_1}(X_1) \prod_{t=2}^{T} A_{Z_{t-1}, Z_t} \, B_{Z_t}(X_t)
$$

### 3.4 Inference: Forward Algorithm
Define forward probability:

$$
\alpha_t(i) = P(X_{1:t}, Z_t=i)
$$

Recursive relation:

$$
\alpha_t(j) = \left( \sum_i \alpha_{t-1}(i) A_{ij} \right) B_j(X_t)
$$

### 3.5 Decoding: Viterbi Algorithm
To find the **most likely hidden state sequence**:

$$
\delta_t(j) = \max_{Z_{1:t-1}} P(Z_{1:t-1}, Z_t=j, X_{1:t})
$$

### 3.6 Parameter Estimation: Baum-Welch Algorithm
Uses **Expectation-Maximization (EM)**:
- E-step: Compute expected counts using forward-backward.  
- M-step: Re-estimate $A, B, \pi$ by maximizing likelihood.  

<br>

## 4. Latent Dirichlet Allocation (LDA)

### 4.1 Generative Process
LDA is a **topic model**. Each document is a mixture of topics, each topic is a distribution over words.  

Generative steps for a document $d$:
1. Choose topic distribution $\theta_d \sim \text{Dirichlet}(\alpha)$  
2. For each word:
   - Choose topic $z_{d,n} \sim \text{Multinomial}(\theta_d)$  
   - Choose word $w_{d,n} \sim \text{Multinomial}(\beta_{z_{d,n}})$  

### 4.2 Dirichlet Distribution
Dirichlet prior over topic distribution $\theta$:

$$
p(\theta \mid \alpha) = \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_i^{\alpha_i - 1}
$$

where $B(\alpha)$ is the Beta function (normalizing constant).

### 4.3 Joint Probability of LDA
For a corpus:

$$
P(W, Z, \Theta \mid \alpha, \beta) = \prod_d P(\theta_d \mid \alpha) \prod_n P(z_{d,n} \mid \theta_d) P(w_{d,n} \mid z_{d,n}, \beta)
$$

### 4.4 Variational Inference (High-level)
Since exact posterior is intractable:

$$
p(\theta, z \mid w, \alpha, \beta) \approx q(\theta, z \mid \gamma, \phi)
$$

We optimize parameters $(\gamma, \phi)$ to minimize KL-divergence.

### 4.5 Gibbs Sampling for LDA
Collapsed Gibbs sampling samples topic assignments $z_{d,n}$:

$$
P(z_{d,n}=k \mid z_{-d,n}, w, \alpha, \beta) \propto (n_{d,k}^{-dn} + \alpha_k) \cdot \frac{n_{k,w_{d,n}}^{-dn} + \beta}{n_k^{-dn} + V\beta}
$$

where:
- $n_{d,k}^{-dn}$: number of words in doc $d$ assigned to topic $k$ (excluding current).  
- $n_{k,w}^{-dn}$: number of times word $w$ assigned to topic $k$.  
- $n_k$: total number of words assigned to topic $k$.  
