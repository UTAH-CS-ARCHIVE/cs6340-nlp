### **[Lecture Note] The Role of Mathematics in Natural Language Processing**

> Written by Hongseo Jang


#### **1. A Brief History of Natural Language Processing (NLP)**

Natural Language Processing (NLP) is a field of artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. The history of NLP can be traced back to the mid-20th century, with early efforts focusing on machine translation.

- **The Early Days (1950s-1960s):** The Georgetown-IBM experiment in 1954 demonstrated the possibility of automatic translation, albeit in a very limited capacity. This era was dominated by rule-based systems, where linguists and computer scientists manually crafted grammatical rules to process language. A notable example from this period is ELIZA, a chatbot created at MIT that simulated a Rogerian psychotherapist.

- **The Statistical Revolution (1980s-1990s):** The limitations of rule-based systems became apparent as they struggled with the ambiguity and complexity of human language. This led to a paradigm shift towards statistical methods. Instead of relying on handcrafted rules, researchers began to use statistical models trained on large corpora of text. This approach proved to be more robust and scalable.

- **The Rise of Machine Learning and Deep Learning (2000s-Present):** The advent of the internet and the availability of vast amounts of text data fueled the development of machine learning-based NLP. More recently, deep learning techniques, particularly neural networks, have achieved state-of-the-art results on a wide range of NLP tasks. Models like Recurrent Neural Networks (RNNs) and Transformers have become the standard for many applications.

<br>

#### **2. Major Tasks and Applications of NLP**

NLP encompasses a wide range of tasks and has numerous real-world applications.

**Major Tasks:**

* **Machine Translation:** Automatically translating text from one language to another. Early systems were rule-based, but modern approaches like Neural Machine Translation (NMT) have significantly improved translation quality.
* **Sentiment Analysis:** Determining the sentiment or emotion expressed in a piece of text (e.g., positive, negative, neutral). This is widely used for analyzing customer reviews, social media comments, and brand monitoring.
* **Text Classification:** Categorizing text into predefined classes. Examples include spam detection in emails, topic labeling of news articles, and intent classification in chatbots.
* **Named Entity Recognition (NER):** Identifying and classifying named entities in text, such as names of people, organizations, locations, and dates.
* **Question Answering (QA):** Building systems that can automatically answer questions posed in natural language.
* **Text Summarization:** Generating a concise and coherent summary of a longer text document.
* **Speech Recognition:** Converting spoken language into written text.

**Applications:**

* **Search Engines:** NLP helps search engines understand the intent behind a user's query to provide more relevant results.
* **Chatbots and Virtual Assistants:** Powering conversational agents like Siri, Alexa, and Google Assistant.
* **Social Media Monitoring:** Analyzing social media data to understand public opinion and trends.
* **Healthcare:** Extracting information from clinical notes and medical records to improve patient care and research.
* **Finance:** Analyzing financial news and reports for market prediction and risk assessment.

<br>

#### **3. The Importance of Mathematical Representation**

To enable computers to process and understand human language, we first need to represent it in a way that they can work with. This is where mathematics plays a crucial role. We need to convert unstructured text data into a structured, numerical format.

The core idea is to represent words, sentences, or documents as numerical vectors. This process is known as **vectorization** or **text embedding**. Once we have these vector representations, we can apply various mathematical operations and machine learning algorithms to them.

<br>

#### **4. Key Mathematical Concepts in NLP**

Probability, statistics, and linear algebra are the foundational mathematical disciplines for modern NLP.

**a) Linear Algebra: Vector Spaces**

Linear algebra is fundamental to how we represent and manipulate text data.

* **Vector Space Model (VSM):** In NLP, we often represent documents as vectors in a high-dimensional space. Each dimension of the vector corresponds to a unique word in the vocabulary. The value in each dimension can be a simple word count, or a more sophisticated weighting like TF-IDF (Term Frequency-Inverse Document Frequency).

    The TF-IDF score for a term *t* in a document *d* is calculated as:

    $$
    \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
    $$

    where:
    * $\text{TF}(t, d)$ is the term frequency of *t* in *d*.
    * $\text{IDF}(t)$ is the inverse document frequency of *t*, calculated as:

        $$
        \text{IDF}(t) = \log\left(\frac{N}{|\{d \in D : t \in d\}|}\right)
        $$

        Here, *N* is the total number of documents in the corpus *D*.

* **Word Embeddings:** More advanced techniques like Word2Vec and GloVe learn dense vector representations of words, capturing semantic relationships between them. For instance, the vectors for "king" and "queen" would be closer to each other in the vector space than the vectors for "king" and "apple".

**b) Probability and Statistics**

Probability and statistics are essential for modeling the inherent uncertainty and variability in language.

* **Probability Variables:** In the context of NLP, a random variable can represent various linguistic phenomena. For example, a random variable *W* could represent the next word in a sentence, and its possible outcomes would be all the words in the vocabulary.

* **Conditional Probability:** This is a cornerstone of many NLP models, especially in language modeling. A language model aims to predict the probability of a sequence of words. For a sequence of words $w_1, w_2, ..., w_n$, its probability is calculated using the chain rule of probability:

    $$
    P(w_1, w_2, ..., w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) ... P(w_n | w_1, ..., w_{n-1})
    $$

    This can be expressed more compactly as:

    $$
    P(w_1, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})
    $$

    This formula tells us that the probability of a sequence of words is the product of the conditional probabilities of each word given the preceding words. This concept is fundamental to tasks like machine translation and speech recognition. For example, a speech recognition system might try to determine the most likely sequence of words given a particular audio signal, which involves calculating the conditional probability of the word sequence given the audio.