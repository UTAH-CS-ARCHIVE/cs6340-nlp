### **[Lecture Note] Optimization of Word2Vec**

> Written by Hongseo Jang

### CBOW (Continuous Bag-of-Words) Model

The CBOW model flips the objective of the Skip-gram model. Instead of using a central word to predict its context, CBOW uses the context words to predict a single central target word.

* **Core Idea:**
    * **Skip-gram:** one word in -> multiple words out. (e.g., given "cat", predict "the", "sat", "on", "mat")
    * **CBOW:** multiple words in -> one word out. (e.g., given "the", "sat", "on", "mat", predict "cat")
    The "Bag-of-Words" part of the name signifies that the order of the context words does not influence the prediction.

* **Model Architecture & Mathematical Steps:**
    Let's consider a context window of size $C=4$ (two words before and two words after the target) and a vocabulary of size $V$.
    1.  **Input Word Vectors:** For a set of context words $\{w_{c_1}, w_{c_2}, \dots, w_{c_C}\}$, we retrieve their corresponding input word vectors $\{v_{c_1}, v_{c_2}, \dots, v_{c_C}\}$ from an input embedding matrix $W$ of size $V \times D$.

    2.  **Vector Averaging:** The defining operation of CBOW is to combine the context vectors into a single vector, $v_{context}$, by averaging them. This is the step that discards word order.
        $$
        v_{context} = \frac{1}{C} \sum_{i=1}^{C} v_{c_i}
        $$

    3.  **Score Calculation:** This aggregated context vector is then used to compute a score vector $z \in \mathbb{R}^V$. We use an output embedding matrix $W'$ (of size $D \times V$). The score for the $j$-th word in the vocabulary is the dot product of its output vector $u_j$ and the context vector $v_{context}$.
        $$
        z_j = u_j^T v_{context}
        $$

    4.  **Probability Distribution:** The scores are converted into a probability distribution $\hat{y}$ using the standard softmax function. This gives the probability of each vocabulary word being the center word, given the context.
        $$
        \hat{y}_j = P(w_j | \text{context}) = \frac{\exp(z_j)}{\sum_{k=1}^{V} \exp(z_k)}
        $$
    The model is then trained using cross-entropy loss to maximize the probability of the true target word.

* **CBOW vs. Skip-gram:**
    * **Training Speed:** CBOW is generally faster. For each training window, it performs one prediction, whereas Skip-gram performs $C$ predictions.
    * **Performance:** The best choice depends on the task. Skip-gram is known to perform better with infrequent words, as it learns a representation for each specific context-target pair. CBOW's averaging operation smooths over the context, which can be more beneficial for representing frequent words.

<br>

### Hierarchical Softmax: An Efficient Alternative to Full Softmax

A major computational bottleneck in both CBOW and Skip-gram is the softmax calculation. The denominator requires a sum over the entire vocabulary $V$, which can be computationally prohibitive (e.g., if $V > 100,000$). Hierarchical Softmax offers a clever solution by changing the structure of the problem.

* **Core Idea:** Instead of a single, flat, $V$-way classification, Hierarchical Softmax reframes the task as a sequence of binary (left/right) decisions. It organizes the entire vocabulary into a **binary tree**, where each of the $V$ words is a unique leaf node.

* **Tree Structure using Huffman Coding:**
    To make this efficient, we want the paths to frequent words to be short. **Huffman coding** provides an optimal way to construct such a tree. It analyzes the unigram frequencies of words in the corpus and builds a binary tree where the most frequent words have the shortest unique paths from the root to their leaf node.

* **How it Works Mathematically:**
    1.  **Path as Code:** Every word in the vocabulary is now identified by its unique path from the root. For example, the word "cat" might be reached by the path [left, right, left].

    2.  **Probability as a Product of Decisions:** The probability of arriving at a specific target word $w$ is the product of the probabilities of making the correct binary decision at each of the internal nodes along its path.
        $$
        P(w | v_{in}) = \prod_{j=1}^{L(w)-1} P(d_j | n(w, j), v_{in})
        $$
        * $v_{in}$ is the input vector (e.g., $v_{context}$ for CBOW or the center word vector for Skip-gram).
        * $n(w, j)$ is the $j$-th internal node on the path to word $w$. Each internal node has its own learned vector representation $u_{n(w,j)}$.
        * $L(w)$ is the length of the path to word $w$.
        * $d_j$ represents the choice made at node $j$ (e.g., left or right).

    3.  **Binary Decision with Sigmoid:** At each internal node $n$, a binary logistic regression model makes a decision. The probability of taking a specific path (e.g., "go right") is calculated using the sigmoid function:
        $$
        P(\text{go right} | n, v_{in}) = \sigma(u_n^T v_{in})
        $$
        The probability of going left is simply $1 - \sigma(u_n^T v_{in})$. A special variable `[[ch]]` is used in the implementation to determine whether to add or subtract in the exponent (e.g., `[[ch]]` = 1 for the "left" child, -1 for the "right" child) to simplify the calculation into a single formula.

* **Computational Advantage:** The number of internal nodes on a path to a word is, on average, $O(\log_2 V)$. Therefore, instead of updating $V$ output vectors for each training sample, we only need to evaluate and update approximately $\log_2 V$ vectors corresponding to the nodes on the path. This provides a massive speedup and makes training on large vocabularies feasible.