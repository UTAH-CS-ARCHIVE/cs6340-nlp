### **[Lecture Note] LSTM and GRU (Long Short-Term Memory & Gated Recurrent Unit)**

> Written by Hongseo Jang

### The LSTM Cell: Fine-Grained Memory Control

The key innovation of the LSTM is the introduction of a dedicated **cell state**, $c_t$. Think of it as a memory "conveyor belt" that runs parallel to the main sequence processing. It can carry information across many time steps with minimal disturbance. This flow is regulated by a system of three sophisticated gates.

* **Core Components:**
    1.  **Forget Gate ($f_t$):** Decides what fraction of the previous cell state, $c_{t-1}$, should be forgotten.
    2.  **Input Gate ($i_t$):** Decides what fraction of the new "candidate" information, $\tilde{c}_t$, should be added to the cell state.
    3.  **Output Gate ($o_t$):** Decides what fraction of the updated cell state, $c_t$, should be exposed as the hidden state, $h_t$.

* **Mathematical Formulation:**
    At each time step $t$, given the input $x_t$ and previous hidden state $h_{t-1}$:

    1.  **Forget Gate:** A sigmoid layer looks at $h_{t-1}$ and $x_t$ and outputs a number between 0 and 1 for each number in the cell state $c_{t-1}$. A 1 represents "completely keep this," while a 0 represents "completely get rid of this."
        $$
        f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
        $$

    2.  **Input Gate & Candidate State:** The input gate decides which values we'll update. A tanh layer creates a vector of new candidate values, $\tilde{c}_t$, that could be added to the state.
        $$
        i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
        $$
        $$
        \tilde{c}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)
        $$

    3.  **Cell State Update:** The old cell state $c_{t-1}$ is multiplied by the forget gate $f_t$, and the new candidate state $\tilde{c}_t$ is multiplied by the input gate $i_t$. These two are then added together.
        $$
        c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
        $$

    4.  **Output Gate & Hidden State Update:** The output gate decides what to output. The cell state is passed through a tanh (to push values between -1 and 1) and then multiplied by the output of the sigmoid gate.
        $$
        o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
        $$
        $$
        h_t = o_t \odot \tanh(c_t)
        $$

<br>

### The GRU Cell: A Simplified and Efficient Design

The GRU was introduced as a simpler alternative to the LSTM. Its main goal is to provide similar functionality with a more streamlined architecture, making it computationally more efficient.

* **Core Changes from LSTM:**
    1.  **Combined State:** The cell state and hidden state are merged into a single state vector, $h_t$.
    2.  **Reduced Gates:** The forget and input gates are combined into a single **update gate ($z_t$)**. A new **reset gate ($r_t$)** is also introduced.

* **Mathematical Formulation:**
    At each time step $t$, given the input $x_t$ and previous hidden state $h_{t-1}$:

    1.  **Reset Gate ($r_t$):** This gate determines how to combine the new input with the previous memory. Specifically, it decides how much of the previous hidden state to forget.
        $$
        r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)
        $$

    2.  **Update Gate ($z_t$):** This gate determines how much of the previous memory to keep around. It acts similarly to the forget gate in LSTM.
        $$
        z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)
        $$

    3.  **Candidate Hidden State:** A new candidate state is computed. Crucially, the reset gate controls how much influence the previous state $h_{t-1}$ has on this candidate. If $r_t$ is close to 0, the candidate is computed almost entirely from the current input $x_t$.
        $$
        \tilde{h}_t = \tanh(W_h [r_t \odot h_{t-1}, x_t] + b_h)
        $$

    4.  **Hidden State Update:** The update gate $z_t$ directly controls the interpolation between the old state $h_{t-1}$ and the new candidate state $\tilde{h}_t$. If $z_t$ is close to 1, the new state is mostly the candidate; if it's close to 0, the old state is mostly preserved.
        $$
        h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
        $$

<br>

### Comparative Analysis: LSTM vs. GRU

| Feature                | LSTM                                                              | GRU                                                                    | Analysis                                                                                                                                                                                            |
| ---------------------- | ----------------------------------------------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **State Vectors** | Two: Cell State ($c_t$) and Hidden State ($h_t$)                  | One: Hidden State ($h_t$)                                              | LSTM has a decoupled memory state ($c_t$) and output state ($h_t$), potentially allowing for more nuanced control. GRU's unified state is simpler.                                                       |
| **Gates** | Three: Forget ($f_t$), Input ($i_t$), Output ($o_t$)              | Two: Reset ($r_t$), Update ($z_t$)                                     | GRU is computationally cheaper due to fewer gates and thus fewer parameters.                                                                                                                         |
| **Computational Cost** | Higher                                                            | Lower                                                                  | GRU trains faster and requires less memory per cell, making it attractive for very large models or datasets.                                                                                        |
| **Performance** | No clear winner; highly task-dependent                            | No clear winner; highly task-dependent                                 | Empirically, neither consistently outperforms the other. LSTM might be better for tasks requiring more precise memory control, while GRU's efficiency and lower parameter count can help on smaller datasets. |

#### A Note on Gradient Flow (The Jacobian Perspective)

The reason both models solve the vanishing gradient problem lies in the **additive nature of their state updates**. In a vanilla RNN, the gradient is repeatedly multiplied by the same weight matrix during backpropagation. In LSTM, the backpropagated gradient for the cell state is multiplied by the forget gate, $f_t$.
$$
\frac{\partial c_t}{\partial c_{t-1}} = f_t
$$
Similarly, in GRU, the gradient with respect to the previous state is scaled by $(1-z_t)$. Since $f_t$ and $z_t$ are dynamic vectors that change at each time step, the network can **learn to set them close to 1**, creating a direct, uninterrupted path for gradients to flow backward. This prevents the Jacobian matrix of the recurrence from systematically shrinking the gradients to zero, thus preserving long-term dependencies.

#### LSTM Variant: Peephole Connections
Standard LSTM gates compute their values based on $h_{t-1}$ and $x_t$. A popular variant called **Peephole LSTM** allows the gates to "peek" at the cell state itself, providing them with more context about the current memory.
$$
f_t = \sigma(W_f [h_{t-1}, x_t] + W_{fc} \odot c_{t-1} + b_f)
$$
$$
i_t = \sigma(W_i [h_{t-1}, x_t] + W_{ic} \odot c_{t-1} + b_i)
$$
This can be beneficial for tasks where the precise value of the memory cell is important for making gating decisions.