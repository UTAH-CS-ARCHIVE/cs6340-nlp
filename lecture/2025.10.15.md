### **[Lecture Note] Neural Network Basics and the Backpropagation Algorithm**

> Written by Hongseo Jang

### Part 1: A Zoo of Activation Functions

The activation function is the component that introduces **non-linearity** into a neural network. Without it, a multi-layered network would mathematically collapse into a single linear transformation, unable to learn complex patterns. While sigmoid is a classic choice, it suffers from a major drawback.

**The Vanishing Gradient Problem:**
Both sigmoid and tanh functions have horizontal asymptotes. When the input to these functions is very large or very small, the function saturates, and its derivative (gradient) approaches zero. In deep networks, these small gradients are multiplied together during backpropagation, causing the overall gradient signal to "vanish" for the earlier layers. This effectively stops them from learning. Modern activation functions are designed to mitigate this.

#### 1. ReLU (Rectified Linear Unit)
ReLU is the most common default activation function used in deep learning today.
* **Mathematical Formula:**
    $$
    f(x) = \max(0, x)
    $$
* **Pros:**
    * **Computationally Efficient:** It's a simple thresholding operation, much faster than computing exponentials (like in sigmoid).
    * **Alleviates Vanishing Gradients:** For positive inputs ($x > 0$), the derivative is a constant 1. This means the gradient does not shrink as it is backpropagated, allowing deeper networks to train effectively.
    * **Induces Sparsity:** Since all negative inputs are mapped to zero, it can lead to sparse representations where some neurons are inactive, which can be computationally and representationally efficient.
* **Cons:**
    * **The "Dying ReLU" Problem:** If a neuron's weights are updated such that its input is always negative, it will always output 0. Consequently, the gradient flowing through it will also always be 0. This neuron becomes "stuck" and can no longer learn.

#### 2. Leaky ReLU
Leaky ReLU is a simple but effective attempt to fix the Dying ReLU problem.
* **Mathematical Formula:** It introduces a small, non-zero slope for negative inputs.
    $$
    f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases}
    $$
    where $\alpha$ is a small, fixed hyperparameter like 0.01.
* **Pros:** By ensuring the gradient is never zero for negative inputs, it prevents neurons from dying.
* **Cons:** The performance can be sensitive to the choice of $\alpha$. A variant, **PReLU (Parametric ReLU)**, treats $\alpha$ as a learnable parameter.

#### 3. ELU (Exponential Linear Unit)
ELU is another alternative that aims to combine the best qualities of ReLU-like functions.
* **Mathematical Formula:**
    $$
    f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \le 0 \end{cases}
    $$
* **Pros:**
    * Solves the Dying ReLU problem.
    * For negative inputs, the function saturates to $-\alpha$, which can help push the mean activation of the layer closer to zero. This property, similar to what Batch Normalization achieves, can speed up learning.
* **Cons:** It is more computationally intensive than ReLU due to the exponential operation.

<br>

### Part 2: Advanced Optimization Algorithms

Standard Stochastic Gradient Descent (SGD) has limitations: it can be slow to navigate flat regions of the loss landscape and can oscillate inefficiently in narrow "ravines." Advanced optimizers aim to overcome these issues.

#### 1. Momentum
This method is designed to accelerate SGD in the relevant direction and dampen oscillations.
* **Intuition:** Imagine a ball rolling down a hill. It builds up momentum, allowing it to pass over small bumps and speed up in consistent downhill directions.
* **Mathematical Update:** We maintain a "velocity" vector $m_t$, which is an exponentially decaying moving average of past gradients.
    $$
    m_t = \beta m_{t-1} + (1 - \beta) g_t
    $$
    $$
    \theta_{t+1} = \theta_t - \eta m_t
    $$
    Here, $g_t$ is the current gradient, $\eta$ is the learning rate, and $\beta$ is the momentum term (e.g., 0.9). The velocity $m_t$ smooths out the gradient updates over time.

#### 2. AdaGrad (Adaptive Gradient)
AdaGrad adapts the learning rate for each parameter individually, performing larger updates for infrequent parameters and smaller updates for frequent ones.
* **Intuition:** This is particularly useful for sparse data, like the word counts in NLP.
* **Mathematical Update:** It accumulates the sum of squared gradients for each parameter.
    $$
    v_t = v_{t-1} + g_t^2
    $$
    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t
    $$
    The term $v_t$ scales the learning rate inversely to the historical magnitude of the gradient.
* **Limitation:** The accumulated sum $v_t$ only ever increases. This causes the learning rate to monotonically decrease, eventually becoming so small that learning stops prematurely.

#### 3. RMSProp (Root Mean Square Propagation)
RMSProp resolves AdaGrad's diminishing learning rate issue by using an exponentially decaying average of squared gradients instead of a cumulative sum.
* **Intuition:** Keep the adaptive learning rate idea but prevent it from vanishing.
* **Mathematical Update:**
    $$
    v_t = \beta v_{t-1} + (1 - \beta) g_t^2
    $$
    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t
    $$
    This looks very similar to AdaGrad, but the moving average (controlled by $\beta$) ensures that $v_t$ is an estimate based on recent gradients, not the entire history.

#### 4. Adam (Adaptive Moment Estimation)
Adam is the most popular and often the default choice for an optimizer. It combines the ideas of Momentum and RMSProp.
* **Intuition:** Use the momentum concept to determine the update direction and the RMSProp concept to adapt the step size for each parameter.
* **Mathematical Update:** It keeps track of two moving averages:
    1.  **First Moment (the mean of gradients, like Momentum):**
        $$
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
        $$
    2.  **Second Moment (the uncentered variance of gradients, like RMSProp):**
        $$
        v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
        $$
    Since $m_t$ and $v_t$ are initialized to zero, they are biased towards zero, especially during the initial steps. Adam performs a **bias correction** to counteract this:
    $$
    \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \quad , \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    $$
    The final parameter update uses these corrected estimates:
    $$
    \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
    $$