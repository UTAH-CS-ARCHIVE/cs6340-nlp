### **[Lecture Note] Probabilistic Language Models**

> Written by Hongseo Jang


#### **1. Introduction to Language Modeling**

A probabilistic language model (LM) is a statistical model that assigns a probability to a sequence of words. The fundamental goal is to capture the regularities and patterns of a language to predict the likelihood of different word sequences. Given a sequence of words $W = (w_1, w_2, \dots, w_n)$, a language model computes its probability, $P(W)$.

Language models are a core component in many NLP applications, including:
- **Machine Translation:** To assess the fluency of a translated sentence. A more probable (fluent) word sequence is preferred.
- **Speech Recognition:** To distinguish between phonetically similar phrases (e.g., "recognize speech" vs. "wreck a nice beach"). The system will favor the sequence with the higher probability.
- **Text Generation:** To predict the next most likely word in a sequence, as seen in autocomplete features and more advanced text generators.

<br>

#### **2. Calculating Sentence Probability: The Chain Rule**

The probability of a sequence of words can be formally calculated using the **Chain Rule of Probability**. For a sentence with *n* words, $w_1, w_2, \dots, w_n$, the joint probability $P(w_1, w_2, \dots, w_n)$ is decomposed as follows:

$$
P(w_1, w_2, \dots, w_n) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times \dots \times P(w_n | w_1, \dots, w_{n-1})
$$

This can be written more compactly using the product notation:

$$
P(w_1, \dots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, \dots, w_{i-1})
$$

Here, $P(w_i | w_1, \dots, w_{i-1})$ represents the probability of the *i*-th word given all the preceding words. While theoretically sound, this approach is practically infeasible for two main reasons:
1.  **Sparsity:** As the context (the sequence of preceding words) gets longer, it becomes increasingly unlikely that we will have ever seen that exact sequence in our training data. This makes it impossible to reliably estimate the conditional probabilities for long contexts.
2.  **Computational Complexity:** Storing and calculating probabilities for every possible word sequence would require an astronomical amount of memory and processing power.

<br>

#### **3. The N-gram Model and the Markov Assumption**

To overcome the limitations of the full chain rule, we introduce the **N-gram model**. This model simplifies the problem by making an assumption about the nature of language. The **Markov Assumption** states that the probability of a word depends only on a limited number of preceding words, not the entire history.

Specifically, in an N-gram model, the probability of a word is approximated by conditioning on the previous $N-1$ words:

$$
P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i | w_{i-(N-1)}, \dots, w_{i-1})
$$

Based on the value of *N*, we can define different models:
-   **Unigram (N=1):** The probability of a word is independent of any context.
    $P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i)$
-   **Bigram (N=2):** The probability of a word depends only on the single preceding word.
    $P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i | w_{i-1})$
-   **Trigram (N=3):** The probability of a word depends on the two preceding words.
    $P(w_i | w_1, \dots, w_{i-1}) \approx P(w_i | w_{i-2}, w_{i-1})$

Using the bigram model as an example, the probability of a sentence is approximated as:

$$
P(w_1, \dots, w_n) \approx \prod_{i=1}^{n} P(w_i | w_{i-1})
$$

For this to work, we often add special start-of-sentence `<s>` and end-of-sentence `</s>` tokens to the vocabulary to properly handle the probabilities of the first and last words.

<br>

#### **4. Parameter Estimation using Maximum Likelihood Estimation (MLE)**

The "parameters" of an N-gram model are the conditional probabilities, such as $P(w_i | w_{i-1})$ for a bigram model. We need to estimate these parameters from a large text corpus. The most straightforward method for this is **Maximum Likelihood Estimation (MLE)**.

The principle of MLE is to choose the parameter values that maximize the likelihood (probability) of the observed training data. For N-gram models, this simplifies to calculating the relative frequencies of observed N-grams in the corpus.

Let's derive the MLE for a bigram model. The parameter we want to estimate is $P(w_i | w_{i-1})$. Intuitively, the best estimate for this probability is the proportion of times we saw the word $w_i$ following the word $w_{i-1}$ in our corpus.

Let $C(w_{i-1}, w_i)$ be the count of the bigram "$w_{i-1} w_i$" occurring in the corpus, and let $C(w_{i-1})$ be the total count of the unigram $w_{i-1}$. The MLE for the bigram probability is:

$$
P_{MLE}(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{\sum_{w} C(w_{i-1}, w)} = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$

The denominator, $\sum_{w} C(w_{i-1}, w)$, represents the sum of counts of all bigrams that start with $w_{i-1}$, which is simply the total count of $w_{i-1}$ itself.

This can be generalized for any N-gram model:

$$
P_{MLE}(w_i | w_{i-N+1}, \dots, w_{i-1}) = \frac{C(w_{i-N+1}, \dots, w_{i-1}, w_i)}{C(w_{i-N+1}, \dots, w_{i-1})}
$$

This estimation is based purely on observed counts, which is both its strength (simplicity) and its weakness. If an N-gram never appears in the training corpus (i.e., its count is zero), the model will assign it a probability of zero. This is problematic, as it means any sentence containing that unseen N-gram will have a total probability of zero, which is often an incorrect assumption. This is known as the **zero-frequency** or **sparsity** problem, which is addressed by smoothing techniques.