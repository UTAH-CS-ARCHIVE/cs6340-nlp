### **[Lecture Note] Vector Representations of Words (Word Embeddings)**

> Written by Hongseo Jang

### Introduction: Bridging Two Worlds of Word Embeddings

In our study of word embeddings, we've seen two main approaches:

1.  **Count-Based Methods (e.g., LSA):** These methods first build a large matrix of co-occurrence statistics for the entire corpus (e.g., a term-document or term-term matrix). Then, they use techniques like Singular Value Decomposition (SVD) to reduce the dimensionality of this matrix, yielding dense word vectors. Their strength is that they leverage **global statistical information** from the entire corpus. However, they are often less effective at capturing the fine-grained semantic relationships needed for tasks like the word analogy task (e.g., "king is to queen as man is to woman").

2.  **Prediction-Based Methods (e.g., Word2Vec):** These methods do not use global co-occurrence counts directly. Instead, they train a model that learns vectors by making predictions within a **local context window**. For example, Skip-gram tries to predict context words given a center word. These models excel at the word analogy task, suggesting they are very good at capturing complex linear relationships between word vectors.

**GloVe (Global Vectors)** was designed to get the best of both worlds. It is fundamentally a count-based model, but its objective function is designed to effectively capture the linear structures that prediction-based models learn so well.

<br>

### The Core Intuition: Ratios of Co-occurrence Probabilities

The key insight behind GloVe is that the **ratios of co-occurrence probabilities**, not the raw probabilities themselves, are the real carriers of meaning.

Let's use the famous example from the original paper. Consider the probe words "ice" and "steam". We want to see how their relationships with other words differ. Let $X_{ij}$ be the number of times word $j$ appears in the context of word $i$. Let $P(j|i) = X_{ij} / X_i$ be the probability of seeing $j$ in $i$'s context.

Now, let's look at the ratio of these probabilities for different probe words:

* Consider a word $k$ like **"solid"**, which is related to "ice" but not "steam". The ratio $P(\text{solid}|\text{ice}) / P(\text{solid}|\text{steam})$ will be very **large**.
* Consider a word $k$ like **"gas"**, which is related to "steam" but not "ice". The ratio $P(\text{gas}|\text{ice}) / P(\text{gas}|\text{steam})$ will be very **small**.
* Consider a word $k$ like **"water"** (related to both) or **"fashion"** (related to neither). For these words, the ratio will be close to **1**.

This demonstrates that ratios of co-occurrence probabilities can effectively distinguish relevant words (like "solid" and "gas") from irrelevant ones and also encode a form of relationship. GloVe is designed to learn word vectors that encode these ratios.

<br>

### Deriving the GloVe Objective Function

The goal is to design a model where vector operations relate directly to these co-occurrence ratios.

**Step 1: The General Model**
Let $w_i$ and $w_j$ be the word vectors and $\tilde{w}_k$ be a separate context vector for a probe word $k$. We want a function $F$ that takes these vectors and outputs the co-occurrence probability ratio:

$$
F(w_i, w_j, \tilde{w}_k) \approx \frac{P(k|i)}{P(k|j)}
$$

**Step 2: Encoding the Ratio in Vector Space**
The authors argue that to capture the linear relationships seen in Word2Vec, the function $F$ should operate on the *difference* between the target vectors, $w_i - w_j$. Also, to produce a scalar output from the vector inputs, the dot product is a natural choice. This refines our model to:

$$
F((w_i - w_j)^T \tilde{w}_k) \approx \frac{P(k|i)}{P(k|j)}
$$

To maintain the linear structure, the function $F$ should be a homomorphism between the vector space (where operations are additive) and the probability space (where operations are multiplicative, i.e., ratios). The exponential function fits this perfectly: $F = \exp$.

$$
\exp((w_i - w_j)^T \tilde{w}_k) = \frac{\exp(w_i^T \tilde{w}_k)}{\exp(w_j^T \tilde{w}_k)} \approx \frac{P_{ik}}{P_{jk}}
$$

**Step 3: From Ratio to Direct Relationship**
From the equation above, we can reasonably assume a direct correspondence:

$$
\exp(w_i^T \tilde{w}_k) \approx P_{ik} = \frac{X_{ik}}{X_i}
$$

Taking the logarithm of both sides gives a linear relationship:

$$
w_i^T \tilde{w}_k \approx \log(P_{ik}) = \log(X_{ik}) - \log(X_i)
$$

**Step 4: Ensuring Symmetry**
The term $\log(X_i)$ depends only on word $i$, not $k$. It can be absorbed as a bias term $b_i$. However, the equation is still not symmetric. If we were to swap the roles of the word $i$ and the context word $k$, the left side becomes $w_k^T \tilde{w}_i$, but the right side contains terms related to $X_k$. To fix this and make the model symmetric, the authors absorb $\log(X_i)$ into one bias $b_i$ and add another bias $\tilde{b}_k$ for the context word. This leads to the core relationship:

$$
w_i^T \tilde{w}_k + b_i + \tilde{b}_k \approx \log(X_{ik})
$$

**Step 5: The Final Objective Function**
We now have a relationship we want our model to learn. The most common way to enforce this is to minimize the squared error between the two sides. We replace the probe word index $k$ with $j$ for general notation and sum over all possible word pairs in the vocabulary $V$:

$$
J' = \sum_{i,j=1}^{V} (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

This is a good start, but it gives equal importance to all co-occurrences. Infrequent co-occurrences might be noisy, while extremely frequent co-occurrences (e.g., "the" and "is") are not very informative. To solve this, a weighting function $f(X_{ij})$ is introduced.

This gives us the **final GloVe objective function**:

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

The weighting function $f(x)$ is designed to:
* Be zero if $x=0$ (so we don't learn from pairs that never co-occur).
* Down-weight very frequent pairs (like stop-words) to prevent them from dominating the training.
* Give reasonable weight to less frequent pairs, which can carry more specific semantic information.

The specific function used is $f(x) = (\min(1, (x/x_{\max})^\alpha))$, with typical values of $x_{\max}=100$ and $\alpha=3/4$.