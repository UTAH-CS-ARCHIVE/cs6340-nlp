# [Assignment Report] Divergence Measures in Probability Distributions

### **Problem: Comprehensive Calculation and Analysis of Divergence Measures**

**Assignment:**
<br>
Consider two discrete probability distributions, $P$ and $Q$, over the same sample space of three events $\{x_1, x_2, x_3\}$:
* $P = [0.1, 0.2, 0.7]$
* $Q = [0.4, 0.4, 0.2]$

Complete the following tasks. Use the base-2 logarithm ($\log_2$) for all calculations.

1.  **KL Divergence:** Calculate the KL Divergence from $Q$ to $P$, denoted as $D_{KL}(P||Q)$.
2.  **Asymmetry of KL Divergence:** Calculate the KL Divergence from $P$ to $Q$, denoted as $D_{KL}(Q||P)$. Briefly state if your results confirm the asymmetry property of KL Divergence.
3.  **Jensen-Shannon Divergence:** Calculate the Jensen-Shannon Divergence between $P$ and $Q$, denoted as $JSD(P||Q)$. This involves three steps:
    a. First, define the mixture distribution $M = \frac{1}{2}(P+Q)$.
    b. Second, calculate $D_{KL}(P||M)$ and $D_{KL}(Q||M)$.
    c. Finally, use these values to compute the JSD.
4.  **Conceptual Interpretation:** Why is JSD a more suitable choice than KL Divergence for tasks like document clustering, where one needs to measure the similarity between items?

<br>

**Solution:**
<br>
**1. Calculation of $D_{KL}(P||Q)$:**
<br>
The formula is $D_{KL}(P||Q) = \sum_{x} P(x) \log_2\left(\frac{P(x)}{Q(x)}\right)$.
$$
D_{KL}(P||Q) = 0.1 \log_2\left(\frac{0.1}{0.4}\right) + 0.2 \log_2\left(\frac{0.2}{0.4}\right) + 0.7 \log_2\left(\frac{0.7}{0.2}\right)
$$
$$
= 0.1 \log_2(0.25) + 0.2 \log_2(0.5) + 0.7 \log_2(3.5)
$$
$$
= 0.1(-2) + 0.2(-1) + 0.7(1.807)
$$
$$
= -0.2 - 0.2 + 1.2649 = 0.8649
$$
So, $D_{KL}(P||Q) \approx 0.865$ bits.

<br>

**2. Calculation of $D_{KL}(Q||P)$ and Asymmetry:**
<br>
The formula is $D_{KL}(Q||P) = \sum_{x} Q(x) \log_2\left(\frac{Q(x)}{P(x)}\right)$.
$$
D_{KL}(Q||P) = 0.4 \log_2\left(\frac{0.4}{0.1}\right) + 0.4 \log_2\left(\frac{0.4}{0.2}\right) + 0.2 \log_2\left(\frac{0.2}{0.7}\right)
$$
$$
= 0.4 \log_2(4) + 0.4 \log_2(2) + 0.2 \log_2(0.2857)
$$
$$
= 0.4(2) + 0.4(1) + 0.2(-1.807)
$$
$$
= 0.8 + 0.4 - 0.3614 = 0.8386
$$
So, $D_{KL}(Q||P) \approx 0.839$ bits.

Since $D_{KL}(P||Q) \approx 0.865$ and $D_{KL}(Q||P) \approx 0.839$, our results confirm that **KL Divergence is asymmetric**, i.e., $D_{KL}(P||Q) \neq D_{KL}(Q||P)$.

<br>

**3. Calculation of $JSD(P||Q)$:**
<br>
**a. Mixture Distribution M:**
$$
M = \frac{1}{2}(P+Q) = \frac{1}{2}([0.1, 0.2, 0.7] + [0.4, 0.4, 0.2])
$$
$$
M = \frac{1}{2}([0.5, 0.6, 0.9]) = [0.25, 0.3, 0.45]
$$
<br>
**b. KL Divergences to M:**
* **$D_{KL}(P||M)$:**
    $$
    = 0.1 \log_2\left(\frac{0.1}{0.25}\right) + 0.2 \log_2\left(\frac{0.2}{0.3}\right) + 0.7 \log_2\left(\frac{0.7}{0.45}\right)
    $$
    $$
    = 0.1(-1.322) + 0.2(-0.585) + 0.7(0.637) = -0.1322 - 0.117 + 0.4459 = 0.1967
    $$
* **$D_{KL}(Q||M)$:**
    $$
    = 0.4 \log_2\left(\frac{0.4}{0.25}\right) + 0.4 \log_2\left(\frac{0.4}{0.3}\right) + 0.2 \log_2\left(\frac{0.2}{0.45}\right)
    $$
    $$
    = 0.4(0.678) + 0.4(0.415) + 0.2(-1.17) = 0.2712 + 0.166 - 0.234 = 0.2032
    $$
<br>
**c. Final JSD Calculation:**
The formula is $JSD(P||Q) = \frac{1}{2} D_{KL}(P||M) + \frac{1}{2} D_{KL}(Q||M)$.
$$
JSD(P||Q) = \frac{1}{2}(0.1967 + 0.2032) = \frac{1}{2}(0.3999) = 0.19995
$$
So, $JSD(P||Q) \approx 0.200$.

<br>

**4. Conceptual Interpretation:**
<br>
JSD is often preferred over KL Divergence for measuring similarity or distance in tasks like document clustering for two primary reasons:

1.  **Symmetry:** JSD is symmetric, meaning $JSD(P||Q) = JSD(Q||P)$. In clustering, the "distance" from document A to document B should be the same as the distance from B to A. KL Divergence does not satisfy this fundamental property of a distance metric, whereas JSD does.
2.  **Finite Value:** JSD is always finite. KL Divergence can be infinite if the model distribution $Q$ assigns zero probability to an event that has a non-zero probability in the true distribution $P$. This can cause numerical instability in algorithms. JSD avoids this by comparing both distributions to a mixture distribution $M$, which guarantees that if an event has non-zero probability in either $P$ or $Q$, it will have a non-zero probability in $M$.