# [Assignment Report] Neural Networks and the Backpropagation Algorithm

### **Problem 1: Forward Propagation in a Single Neuron**

**Assignment:**
<br>
Consider a single neuron with two inputs, $x_1 = 3$ and $x_2 = -2$. The corresponding weights are $w_1 = 0.5$ and $w_2 = 1.5$. The neuron has a bias of $b = -1$. The activation function for this neuron is the sigmoid function, $\sigma(z) = \frac{1}{1+e^{-z}}$.

Calculate the output activation, $a$, of this neuron.

<br>

**Solution:**
<br>
The process involves two steps: calculating the weighted sum $z$, and then applying the activation function $\sigma(z)$.

**Step 1: Calculate the weighted sum (pre-activation), $z$.**
$$
z = (w_1 x_1 + w_2 x_2) + b
$$
$$
z = (0.5 \times 3 + 1.5 \times -2) + (-1)
$$
$$
z = (1.5 - 3.0) - 1 = -1.5 - 1 = -2.5
$$
<br>
**Step 2: Apply the sigmoid activation function.**
$$
a = \sigma(z) = \sigma(-2.5) = \frac{1}{1 + e^{-(-2.5)}} = \frac{1}{1 + e^{2.5}}
$$
Numerically, $e^{2.5} \approx 12.18$.
$$
a \approx \frac{1}{1 + 12.18} = \frac{1}{13.18} \approx 0.0758
$$
The output activation of the neuron is approximately 0.0758.

<br>

---

<br>

### **Problem 2: The Core Mathematical Step of Backpropagation**

**Assignment:**
<br>
The heart of the backpropagation algorithm is the recursive calculation of the error term, $\delta^{[l]}$, for each hidden layer. The formula for calculating the error at layer $l$ based on the error from layer $l+1$ is given by:
$$
\delta^{[l]} = (W^{[l+1]})^T \delta^{[l+1]} \odot \sigma'(z^{[l]})
$$
Explain the specific role of each of the three mathematical components on the right-hand side of this equation: $(W^{[l+1]})^T$, $\delta^{[l+1]}$, and $\sigma'(z^{[l]})$.

<br>

**Solution:**
<br>
Each component in this equation plays a distinct role in propagating the error signal backward through the network.

1.  **$\delta^{[l+1]}$ (Error from the next layer):**
    * This term, $\delta^{[l+1]} = \frac{\partial L}{\partial z^{[l+1]}}$, represents the error signal originating from layer $l+1$ (the layer closer to the output). It quantifies how much each neuron in layer $l+1$ contributed to the overall loss of the network.

2.  **$(W^{[l+1]})^T$ (Transposed Weight Matrix):**
    * This is the transpose of the weight matrix that connects layer $l$ to layer $l+1$. Its crucial role is to **propagate and distribute** the error from layer $l+1$ back to the neurons in layer $l$. Each neuron in layer $l$ contributes to multiple neurons in layer $l+1$, and this matrix multiplication correctly aggregates the error signal back to the source neurons, weighted by the strength of the connections ($w_{ij}$) that were used in the forward pass.

3.  **$\sigma'(z^{[l]})$ (Local Gradient of Activation):**
    * This is the derivative of the activation function of layer $l$, evaluated at the pre-activation value $z^{[l]}$. This term scales the backpropagated error based on the neuron's state during the forward pass. If a neuron's activation is in a saturated region (i.e., the activation function is flat, and its derivative is close to zero), it means small changes to its input $z^{[l]}$ would have had little effect on its output. Consequently, the gradient passed backward through this neuron is diminished. This ensures that the weight updates are proportional to how much a neuron was able to influence the network's output.

<br>

---

<br>

### **Problem 3: Final Gradient Calculation for a Hidden Layer**

**Assignment:**
<br>
Using the concepts from the lecture, write the final mathematical expression for the gradient of the loss function $L$ with respect to the weights of the *first hidden layer*, $W^{[1]}$. Your answer should be expressed in terms of the network's input vector $x$ and the error terms ($\delta$) of the layers.

<br>

**Solution:**
<br>
The gradient of the loss with respect to the weights of any given layer is the product of the error term for that layer and the activations that were fed into it from the previous layer.

For the first hidden layer, $W^{[1]}$:
* The error term for this layer is $\delta^{[1]}$.
* The input to this layer is the original input vector of the network, $x$.

Therefore, the gradient of the loss with respect to the weights of the first hidden layer, $\frac{\partial L}{\partial W^{[1]}}$, is given by the outer product of the error term $\delta^{[1]}$ and the input vector $x$. The transpose is used to ensure the resulting matrix has the correct dimensions for the weight update.
$$
\frac{\partial L}{\partial W^{[1]}} = \delta^{[1]} x^T
$$
where $\delta^{[1]}$ is itself calculated from the error of the second layer:
$$
\delta^{[1]} = (W^{[2]})^T \delta^{[2]} \odot \sigma'(z^{[1]})
$$
This demonstrates the full backward propagation of the error signal from the output layer all the way back to the weights connecting the input layer.