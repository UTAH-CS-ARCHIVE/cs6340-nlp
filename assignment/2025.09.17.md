# [Assignment Report] The GloVe Model for Word Embeddings

### **Problem 1: The Core Intuition of Co-occurrence Ratios**

**Assignment:**
<br>
Explain the central intuition behind the GloVe model. Why does it posit that the **ratios** of co-occurrence probabilities are more meaningful for learning word vectors than the raw probabilities themselves? Use the example from the lecture notes involving the probe words "ice" and "steam" and their relationship with other words like "solid," "gas," and "water" to illustrate your explanation.

<br>

**Solution:**
<br>
The central intuition of GloVe is that **ratios of co-occurrence probabilities are the primary carriers of meaning and semantic relationships**. Raw co-occurrence probabilities can be noisy and are often dominated by high-frequency words, failing to highlight the specific relationships between words. Ratios, however, provide a more discriminative and robust signal.

Using the example from the lecture:
* Let $P(k|word)$ be the probability that word $k$ appears in the context of a given `word`.
* We examine the ratio $\frac{P(k|\text{ice})}{P(k|\text{steam})}$.

1.  **For a word like "solid":** "solid" is strongly related to "ice" but not "steam". Therefore, $P(\text{solid}|\text{ice})$ will be high, and $P(\text{solid}|\text{steam})$ will be low. The resulting ratio will be **very large**.
2.  **For a word like "gas":** "gas" is strongly related to "steam" but not "ice". Therefore, $P(\text{gas}|\text{ice})$ will be low, and $P(\text{gas}|\text{steam})$ will be high. The resulting ratio will be **very small**.
3.  **For a word like "water":** "water" is related to both "ice" and "steam". The probabilities in the numerator and denominator will be of similar magnitude, so the ratio will be **close to 1**.
4.  **For an unrelated word like "fashion":** This word is not related to either, so both probabilities will be low, and the ratio will also be **close to 1**.

This shows that the ratio effectively filters out irrelevant words (like "water" and "fashion") and specifically highlights the words that distinguish the meanings of "ice" and "steam" (like "solid" and "gas"). GloVe is designed to learn word vectors that directly encode these meaningful ratios.

<br>

---

<br>

### **Problem 2: The GloVe Objective Function**

**Assignment:**
<br>
The final objective function for the GloVe model is:

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

Explain the specific mathematical purpose of the two main components of this function:
1.  The squared error term: $(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2$
2.  The weighting function: $f(X_{ij})$

<br>

**Solution:**
<br>
**1. The Squared Error Term:**
<br>
This component forms the core of a **weighted least-squares regression model**. The goal is to make the model's prediction as close as possible to the target value derived from the corpus.
* **Model's Prediction:** $w_i^T \tilde{w}_j + b_i + \tilde{b}_j$. This is the dot product between the vector for the main word $i$ ($w_i$) and the vector for the context word $j$ ($\tilde{w}_j$), adjusted by bias terms for each word ($b_i, \tilde{b}_j$).
* **Target Value:** $\log X_{ij}$. This is the logarithm of the number of times word $i$ and word $j$ co-occur in the corpus.
By minimizing the squared difference between the prediction and the target, the training process forces the learned word vectors ($w_i, \tilde{w}_j$) and biases ($b_i, \tilde{b}_j$) to encode the co-occurrence information present in the corpus.

**2. The Weighting Function $f(X_{ij})$:**
<br>
The weighting function is crucial for making the model robust and preventing it from being skewed by the frequency distribution of words in the language. Its purposes are:
* **Handling Zero Co-occurrences:** The function is defined such that $f(0) = 0$. This ensures that pairs of words that never co-occur do not contribute to the loss function, preventing the model from wasting computational effort on an infinite number of zero-count pairs.
* **Down-weighting Frequent Co-occurrences:** Very common word pairs (e.g., "the is", "of the") appear millions of times but are not very informative about the specific meaning of words. The function $f(x)$ is designed to grow slowly for large values of $x$, so these high-frequency pairs are given less weight and do not dominate the training process.
* **Giving Importance to Infrequent Co-occurrences:** Less frequent co-occurrences can be more semantically meaningful (e.g., "computational linguistics"). The function gives these pairs a reasonable weight, ensuring that the model learns from this valuable information without being overly influenced by the potential noise in very rare counts.

<br>

---

<br>

### **Problem 3: The Role of the Exponential Function in the Derivation**

**Assignment:**
<br>
During the derivation of the GloVe model, the authors propose a function $F$ that relates the word vectors to the co-occurrence ratio. They argue that the exponential function is a particularly suitable choice for $F$. What mathematical property of the exponential function makes it ideal for bridging the relationship between vector differences (in the word vector space) and probability ratios (in the co-occurrence space)?

<br>

**Solution:**
<br>
The exponential function is ideal because it is a **homomorphism** that maps addition to multiplication. Specifically, it connects the group of real numbers under addition $(\mathbb{R}, +)$ to the group of positive real numbers under multiplication $(\mathbb{R}^+, \times)$.

This property is expressed as:

$$
\exp(A - B) = \frac{\exp(A)}{\exp(B)}
$$

This is precisely the structure GloVe aims to model:
1.  **Vector Space (Additive Structure):** The relationship between two words is captured by the *difference* of their vectors, e.g., $w_i - w_j$. This is an additive operation.
2.  **Co-occurrence Space (Multiplicative Structure):** The meaningful relationship in the corpus statistics is captured by the *ratio* of their co-occurrence probabilities, e.g., $\frac{P_{ik}}{P_{jk}}$. This is a multiplicative operation.

By choosing $F=\exp$, the model can directly map the additive structure of the vector space to the multiplicative structure of the probability space. This allows the model to learn word vectors where vector differences directly correspond to ratios of co-occurrence probabilities, thus elegantly preserving the linear structures that are known to be effective in tasks like word analogies.