# [Assignment Report] Smoothing Techniques in Language Models

### **Problem 1: Laplace (Add-One) Smoothing Calculation**

**Assignment:**
<br>
You are building a bigram language model. After processing a large corpus, you have the following information:
* The total vocabulary size is $V = 20,000$.
* The count of the unigram "computational" is $C(\text{computational}) = 50$.
* The count of the bigram "computational linguistics" is $C(\text{computational linguistics}) = 12$.
* The bigram "computational neuroscience" was never seen in the corpus.

Using **Laplace (Add-One) Smoothing**, calculate the following probabilities:
1.  The probability of the seen bigram, $P_{Laplace}(\text{linguistics} | \text{computational})$.
2.  The probability of the unseen bigram, $P_{Laplace}(\text{neuroscience} | \text{computational})$.

<br>

**Solution:**
<br>
The formula for Laplace smoothing in a bigram model is:
$$
P_{Laplace}(w_i | w_{i-1}) = \frac{C(w_{i-1}w_i) + 1}{C(w_{i-1}) + V}
$$
<br>

**1. Probability of the Seen Bigram:**
<br>
We substitute the given values into the formula:
* $C(\text{computational linguistics}) = 12$
* $C(\text{computational}) = 50$
* $V = 20,000$

$$
P_{Laplace}(\text{linguistics} | \text{computational}) = \frac{12 + 1}{50 + 20000} = \frac{13}{20050} \approx 0.000648
$$
<br>

**2. Probability of the Unseen Bigram:**
<br>
For the unseen bigram, the count is zero:
* $C(\text{computational neuroscience}) = 0$
* $C(\text{computational}) = 50$
* $V = 20,000$

$$
P_{Laplace}(\text{neuroscience} | \text{computational}) = \frac{0 + 1}{50 + 20000} = \frac{1}{20050} \approx 0.0000498
$$
This demonstrates how Laplace smoothing ensures that even unseen n-grams are assigned a non-zero probability.

<br>

---

<br>

### **Problem 2: Good-Turing Smoothing Adjusted Counts**

**Assignment:**
<br>
You are applying Good-Turing smoothing to a large dataset of n-grams. You have computed the "frequency of frequencies" ($N_c$) for your data. You have the following specific values:
* The number of distinct n-grams that appear exactly 6 times is $N_6 = 500$.
* The number of distinct n-grams that appear exactly 7 times is $N_7 = 420$.
* The number of distinct n-grams that appear exactly once is $N_1 = 1,200,000$.
* The total number of n-gram tokens in the corpus is $N = 30,000,000$.

1.  What is the Good-Turing adjusted count, $c^*$, for an n-gram that originally appeared 6 times (i.e., where $c=6$)?
2.  What is the total probability mass that the Good-Turing model reserves for all n-grams that were never seen in the corpus (i.e., where the count is 0)?

<br>

**Solution:**
<br>
**1. Adjusted Count Calculation:**
<br>
The formula for the Good-Turing adjusted count is:
$$
c^* = (c+1) \frac{N_{c+1}}{N_c}
$$
We are given $c=6$, $N_6 = 500$, and $N_7 = 420$.
$$
c^* = (6+1) \frac{N_7}{N_6} = 7 \times \frac{420}{500} = 7 \times 0.84 = 5.88
$$
The adjusted count is 5.88. This demonstrates how Good-Turing "discounts" the original counts of seen events to save probability mass for unseen events. The model effectively treats an item seen 6 times as if it had been seen 5.88 times.

<br>

**2. Probability Mass for Unseen Events:**
<br>
A key principle of Good-Turing smoothing is that the total probability mass assigned to all unseen events (where $c=0$) is estimated by the proportion of events that were seen exactly once.
<br>
The formula is:
$$
P(\text{unseen events}) = \frac{N_1}{N}
$$
Using the given values:
$$
P(\text{unseen events}) = \frac{1,200,000}{30,000,000} = \frac{12}{300} = \frac{1}{25} = 0.04
$$
The model assigns a total of 4% of the total probability mass to be distributed among all the n-grams that were never observed in the training data.

<br>

---

<br>

### **Problem 3: The Intuition of Kneser-Ney Smoothing**

**Assignment:**
<br>
Explain the core linguistic intuition behind Kneser-Ney smoothing's **continuation probability**. How does this concept make it a more effective smoothing method than simpler approaches that rely on standard unigram probabilities for backoff? Use the example words "Francisco" and "table" to support your explanation.

<br>

**Solution:**
<br>
The core intuition of Kneser-Ney's continuation probability is that **a word's suitability to appear in a new, unseen context is better measured by the diversity of contexts it has already appeared in, rather than its raw frequency.**

A standard unigram model, $P_{MLE}(w) = C(w)/N$, would be used as the backoff (lower-order) distribution in simpler models like Add-k or basic interpolation. This can be misleading.

* Consider the word **"Francisco"**. In a typical corpus, it is very frequent, but almost exclusively appears after the word "San". Its unigram probability might be high, but its contextual diversity is extremely low.
* Consider the word **"table"**. It might be less frequent overall than "Francisco", but it appears in a wide variety of contexts, following words like "on the", "dinner", "periodic", "round", etc.

Kneser-Ney's continuation probability, $P_{continuation}(w)$, is proportional to the number of unique preceding words (contexts) that $w$ has been observed with.
* $P_{continuation}(\text{Francisco})$ would be very low because it has only one major preceding context.
* $P_{continuation}(\text{table})$ would be much higher because it has many preceding contexts.

This makes the continuation probability a far more effective backoff distribution. When the higher-order model (e.g., bigram) fails because it has not seen a sequence like "computational table", the model must "back off" to a lower-order probability for "table". Kneser-Ney correctly reasons that "table" is a much more plausible continuation word in a novel context than "Francisco" is, because it has demonstrated a history of being "promiscuous" with its preceding words. This directly addresses the goal of smoothing: generalizing to unseen but plausible word sequences.