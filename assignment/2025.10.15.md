# [Assignment Report] Activation Functions and Advanced Optimizers

### **Problem 1: The "Dying ReLU" Problem and Its Solution**

**Assignment:**
<br>
1.  Explain the "Dying ReLU" problem. What happens to a neuron's output and, more importantly, its gradient during backpropagation when it "dies"?
2.  The mathematical formula for Leaky ReLU is given by $f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases}$ for a small constant $\alpha > 0$. Explain how this formulation mathematically solves the "Dying ReLU" problem.

<br>

**Solution:**
<br>
**1. The "Dying ReLU" Problem:**
<br>
The "Dying ReLU" problem occurs when a neuron's weights are updated in such a way that the weighted sum of its inputs ($z = w^Tx + b$) is always negative for any input it receives from the dataset.
* **Neuron's Output:** Since the ReLU function is defined as $f(z) = \max(0, z)$, the neuron's output will perpetually be 0.
* **Neuron's Gradient:** The derivative of the ReLU function is 1 for $z>0$ and 0 for $z \le 0$. When a neuron is "dead," its input $z$ is always non-positive, so the local gradient, $\frac{\partial f}{\partial z}$, is always 0. During backpropagation, this zero gradient multiplies with the incoming error signal, causing the gradient for that neuron's weights to also be zero. This means the neuron's weights can no longer be updated, and it effectively stops learning for the remainder of the training process.

<br>

**2. How Leaky ReLU Solves the Problem:**
<br>
Leaky ReLU solves this issue by modifying the function for negative inputs.
* Instead of outputting a flat 0 for all negative inputs, it outputs a value with a small, non-zero slope, $\alpha x$.
* This means the derivative of the Leaky ReLU function is no longer 0 for negative inputs. The derivative is:
    $$
    f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x \le 0 \end{cases}
    $$
* Because $\alpha$ is a small positive constant (e.g., 0.01), there is always a non-zero gradient flowing backward through the neuron, even when its input is negative. This ensures that the neuron's weights can continue to be updated via gradient descent, allowing it to potentially "recover" and start firing again for some inputs.

<br>

---

<br>

### **Problem 2: Comparing AdaGrad and RMSProp**

**Assignment:**
<br>
The update equations for the squared gradient accumulator, $v_t$, for AdaGrad and RMSProp are given below:
* **AdaGrad:** $v_t = v_{t-1} + g_t^2$
* **RMSProp:** $v_t = \beta v_{t-1} + (1 - \beta) g_t^2$

Explain the critical mathematical difference between these two equations. How does the modification introduced in RMSProp address the primary limitation of AdaGrad?

<br>

**Solution:**
<br>
**Mathematical Difference:**
<br>
The critical difference lies in how they accumulate past gradient information.
* **AdaGrad** uses a **cumulative sum**. The variable $v_t$ is the sum of all squared gradients from the beginning of training ($t=0$) up to the current step. This sum is strictly non-decreasing.
* **RMSProp** uses an **exponentially decaying moving average**. The variable $v_t$ is a weighted average of the previous accumulator value ($v_{t-1}$) and the current squared gradient ($g_t^2$). The hyperparameter $\beta$ controls the decay rate, giving more weight to recent gradients and "forgetting" gradients from the distant past.

**Addressing AdaGrad's Limitation:**
<br>
The primary limitation of AdaGrad is its **aggressively diminishing learning rate**. Because the accumulator $v_t$ in AdaGrad only ever increases, the denominator in the update rule, $\sqrt{v_t + \epsilon}$, also only ever increases. This causes the effective learning rate for each parameter to monotonically decrease throughout training, often becoming so small that the model stops learning prematurely.

RMSProp solves this by using the moving average. The accumulator $v_t$ is no longer guaranteed to grow indefinitely; it is an estimate of recent squared gradients. This prevents the denominator from growing uncontrollably, allowing the adaptive learning rate to remain responsive and preventing learning from halting prematurely.

<br>

---

<br>

### **Problem 3: The Components of the Adam Optimizer**

**Assignment:**
<br>
The Adam (Adaptive Moment Estimation) optimizer is widely used because it combines the benefits of other popular optimizers. It maintains two moving averages, $m_t$ and $v_t$:
1.  $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$
2.  $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

Identify which of the simpler optimization algorithms (Momentum, AdaGrad, RMSProp) each of these two estimates is analogous to. Additionally, explain the purpose of the **bias-correction** step that Adam performs.

<br>

**Solution:**
<br>
**Analogies to Simpler Optimizers:**
1.  **$m_t$ (First Moment Estimate):** This is the moving average of the gradients. It is analogous to the **Momentum** optimizer. It helps to accelerate the optimizer in a consistent direction and dampens oscillations, effectively smoothing the gradient updates.
2.  **$v_t$ (Second Moment Estimate):** This is the moving average of the squared gradients. It is analogous to the **RMSProp** optimizer. It provides an adaptive, per-parameter learning rate by scaling the updates based on the historical magnitude of the gradients for that parameter.

**Purpose of Bias Correction:**
<br>
The moving averages $m_t$ and $v_t$ are typically initialized as zero vectors. In the early steps of training, this initialization causes them to be **biased towards zero**. For example, at $t=1$, $m_1 = (1-\beta_1)g_1$, which is a smaller estimate of the gradient than $g_1$ itself.

The bias-correction step is designed to counteract this initialization bias. The formulas are:
$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \quad , \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$
The denominators $(1-\beta^t)$ are small at the beginning of training (when $t$ is small) and approach 1 as $t$ increases. This scaling inflates the estimates of $m_t$ and $v_t$ during the initial timesteps, providing a more accurate and unbiased estimate of the true moments of the gradient distribution right from the start of the training process.