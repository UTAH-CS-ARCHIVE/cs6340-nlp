# [Assignment Report] Linear Algebra in Natural Language Processing

### **Problem 1: Vector Norms Calculation and Interpretation**

**Assignment:**
<br>
Given the vector $v = [4, -3, 0, 12]$ in a 4-dimensional space:
1.  Calculate the $L_2$ norm (Euclidean norm) of vector $v$.
2.  Calculate the $L_1$ norm (Manhattan norm) of vector $v$.
3.  Briefly explain the conceptual difference between these two norms in the context of machine learning regularization.

<br>

**Solution:**
<br>
**1. $L_2$ Norm Calculation:**
<br>
The formula for the $L_2$ norm is $\|v\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}$.
$$
\|v\|_2 = \sqrt{4^2 + (-3)^2 + 0^2 + 12^2} = \sqrt{16 + 9 + 0 + 144} = \sqrt{169} = 13
$$
The $L_2$ norm of vector $v$ is 13. This represents the geometric length of the vector in Euclidean space.

<br>

**2. $L_1$ Norm Calculation:**
<br>
The formula for the $L_1$ norm is $\|v\|_1 = \sum_{i=1}^{n} |v_i|$.
$$
\|v\|_1 = |4| + |-3| + |0| + |12| = 4 + 3 + 0 + 12 = 19
$$
The $L_1$ norm of vector $v$ is 19.

<br>

**3. Conceptual Difference in Regularization:**
<br>
* **$L_2$ Regularization (Ridge):** When used as a penalty term in a model's loss function, the $L_2$ norm penalizes the square of the model's weights. This encourages all weights to become small and distributed, but it rarely forces them to be exactly zero. It is effective at preventing overfitting by reducing model complexity.
* **$L_1$ Regularization (Lasso):** The $L_1$ norm penalizes the absolute value of the weights. This has the property of producing sparse solutions, meaning it can force the weights of less important features to become exactly zero. This makes $L_1$ regularization useful not only for preventing overfitting but also for performing automatic feature selection.

<br>

---

<br>

### **Problem 2: Dot Product and Cosine Similarity**

**Assignment:**
<br>
In a Vector Space Model, two documents are represented by the term frequency vectors:
* $u = [1, 2, 3, 0]$
* $v = [2, 0, 2, 4]$

1.  Calculate the dot product $u \cdot v$.
2.  Calculate the cosine similarity between vectors $u$ and $v$.
3.  Based on the dot product and cosine similarity, what can you infer about the geometric relationship between these two vectors?

<br>

**Solution:**
<br>
**1. Dot Product Calculation:**
<br>
The dot product is calculated as $u \cdot v = \sum_{i=1}^{n} u_i v_i$.
$$
u \cdot v = (1)(2) + (2)(0) + (3)(2) + (0)(4) = 2 + 0 + 6 + 0 = 8
$$
The dot product is 8.

<br>

**2. Cosine Similarity Calculation:**
<br>
The formula for cosine similarity is $\cos(\theta) = \frac{u \cdot v}{\|u\|_2 \|v\|_2}$. First, we must calculate the $L_2$ norm of each vector.

* Norm of $u$:
    $$
    \|u\|_2 = \sqrt{1^2 + 2^2 + 3^2 + 0^2} = \sqrt{1 + 4 + 9 + 0} = \sqrt{14}
    $$
* Norm of $v$:
    $$
    \|v\|_2 = \sqrt{2^2 + 0^2 + 2^2 + 4^2} = \sqrt{4 + 0 + 4 + 16} = \sqrt{24}
    $$
* Now, we compute the cosine similarity:
    $$
    \text{similarity}(u, v) = \frac{8}{\sqrt{14} \sqrt{24}} = \frac{8}{\sqrt{336}} \approx \frac{8}{18.33} \approx 0.436
    $$
<br>

**3. Geometric Interpretation:**
<br>
* Since the dot product is positive ($u \cdot v = 8 > 0$), we can infer that the angle $\theta$ between the vectors is acute (i.e., less than $90^{\circ}$).
* The cosine similarity value of approximately 0.436 confirms this. A value of 0 would mean they are orthogonal (no similarity), and a value of 1 would mean they are identical in orientation. A value of 0.436 indicates a moderate degree of similarity, suggesting the documents share some common themes but are not identical in their content distribution.

<br>

---

<br>

### **Problem 3: Matrix Multiplication as a Linear Transformation**

**Assignment:**
<br>
The lecture notes describe matrix multiplication as a **linear transformation**. In a neural network for NLP, an input word embedding vector is often multiplied by a weight matrix. For example, if $x$ is an input vector of size $d_{in}$ and $W$ is a weight matrix of size $d_{out} \times d_{in}$, the transformation is $h = Wx$.

Explain what this operation achieves conceptually. What is being transformed, and why is this transformation a fundamental step in how neural networks process language?

<br>

**Solution:**
<br>
**What is being transformed:**
<br>
The object being transformed is the input vector $x$, which represents a piece of text (e.g., a word or sentence) in a specific vector space of dimension $d_{in}$. The matrix multiplication $Wx$ transforms this vector $x$ into a new vector $h$ in a different vector space of dimension $d_{out}$.

**Purpose of the Transformation:**
<br>
This transformation is fundamental because it allows the network to learn new, more useful representations of the input data. The weight matrix $W$ contains learnable parameters. During training, the network adjusts the values in $W$ to perform a linear transformation (a combination of rotation, scaling, and shearing) that projects the input data into a new space where the task becomes easier to solve.

For example, this new representation $h$ might:
* Emphasize features of the input that are important for a classification task.
* Separate vectors of different classes to make them linearly separable.
* Combine input features to create more abstract, higher-level features.

In essence, the matrix multiplication is not just a calculation; it is the core mechanism by which a neural network layer **learns to re-represent the input data** in a way that is progressively more tailored to solving the final objective.