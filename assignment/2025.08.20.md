# [Assignment] Statistical Models in NLP


## Part I. N-gram Models

### 1. Markov Assumption
The **Markov assumption** in language modeling states that the probability of a word depends only on the previous $n-1$ words:

$$
P(w_t \mid w_1, w_2, \dots, w_{t-1}) \approx P(w_t \mid w_{t-n+1}, \dots, w_{t-1})
$$
For bigrams ($n=2$):
$$
P(w_t \mid w_1, \dots, w_{t-1}) \approx P(w_t \mid w_{t-1})
$$

- **Simplification:** It reduces dependency length, making estimation feasible with limited data.
- **Drawbacks:**
  1. **Data sparsity**: Many $n$-grams never occur, leading to zero probabilities.
  2. **Short context**: Important long-distance dependencies (e.g., subject–verb agreement) are ignored.

---

### 2. Training Corpus
Sentences:
1. “the cat sat on the mat”  
2. “the cat ate fish”

**Vocabulary** = {the, cat, sat, on, mat, ate, fish}  
$|V|=7$

#### a) $P(\text{sat} \mid \text{cat})$ with bigram MLE
Formula:

$$
P_{\text{MLE}}(w_i \mid w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$

Counts:
- $C(\text{cat}, \text{sat}) = 1$  
- $C(\text{cat}, \text{ate}) = 1$  
- $C(\text{cat}) = 2$

Thus:

$$
P(\text{sat} \mid \text{cat}) = \frac{1}{2}
$$

#### b) $P(\text{ate} \mid \text{cat})$ with add-one smoothing
Formula:

$$
P_{\text{Laplace}}(w_i \mid w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |V|}
$$

Counts:
- $C(\text{cat}, \text{ate}) = 1$
- $C(\text{cat}) = 2$
- $|V| = 7$

So:

$$
P(\text{ate} \mid \text{cat}) = \frac{1 + 1}{2 + 7} = \frac{2}{9}
$$

---

### 3. Why Increasing $n$ Is Not Always Better
- **Data sparsity:** Larger $n$ means more unique $n$-grams, many unseen.  
- **Memory/efficiency:** Model size grows exponentially with $n$.  
- **Overfitting:** With small data, high-order $n$-grams memorize training but fail to generalize.  
- **Diminishing returns:** Beyond trigrams, marginal performance improvements decrease.

<br>

## Part II. Markov Chains

### 1. Stationary Distribution Derivation
Suppose states = {Rainy (R), Sunny (S)}  
Transition matrix:

$$
P =
\begin{bmatrix}
p_{RR} & p_{RS} \\
p_{SR} & p_{SS}
\end{bmatrix}
$$

Stationary distribution $\pi = [\pi_R, \pi_S]$ satisfies:

$$
\pi P = \pi, \quad \pi_R + \pi_S = 1
$$

System:

$$
\pi_R = \pi_R p_{RR} + \pi_S p_{SR}
$$
$$
\pi_S = \pi_R p_{RS} + \pi_S p_{SS}
$$

Solve for $\pi_R, \pi_S$ subject to normalization.

### 2. Interpretation
- The stationary distribution gives the **long-run proportion of time** spent in each weather state.
- Example: $\pi_R = 0.4, \pi_S = 0.6$ → in the long run, 40% rainy, 60% sunny.

### 3. NLP Example with Simple Markov Chains
- **Application:** Character-level text generation (predict next character from previous one).
- **Limitation:** Generated text captures local structure but lacks long-range coherence.

<br>

## Part III. Hidden Markov Models (HMMs)

### 1. Three Key Problems
1. **Evaluation**: Compute probability of observation sequence.  
   - Algorithm: **Forward algorithm**.  
2. **Decoding**: Find most likely hidden state sequence.  
   - Algorithm: **Viterbi algorithm**.  
3. **Learning**: Estimate parameters from data.  
   - Algorithm: **Baum-Welch (EM)**.

---

### 2. Forward Algorithm Recurrence
For observation sequence $O = (o_1, o_2, o_3)$, states $S=\{s_1,s_2\}$:

Initialization:

$$
\alpha_1(j) = \pi_j \cdot b_j(o_1)
$$

Recursion:

$$
\alpha_t(j) = \left(\sum_{i=1}^N \alpha_{t-1}(i) \cdot a_{ij}\right) \cdot b_j(o_t)
$$

Termination:

$$
P(O) = \sum_j \alpha_T(j)
$$

---

### 3. Viterbi vs Forward
- **Forward algorithm:** Computes **total probability** of observations.  
- **Viterbi algorithm:** Finds **single most likely path** of hidden states.  
- Outputs differ: probability mass vs. optimal sequence.

---

### 4. EM (Baum-Welch) for HMM Training
- **Why needed:** Exact MLE is intractable due to hidden states → exponential paths.  
- **Approach:** EM alternates:  
  - E-step: compute expected sufficient statistics using forward–backward.  
  - M-step: re-estimate transition and emission probabilities.  
- Provides a **locally optimal** estimate of parameters.

<br>

## Part IV. Latent Dirichlet Allocation (LDA)

### 1. Dirichlet as Prior
- Each document has a topic distribution $\theta$.  
- Dirichlet prior ensures:
  - $\theta$ lies on a simplex (sums to 1).  
  - Provides control over sparsity vs. uniformity.  
- Natural conjugate prior for multinomial topic assignments.

---

### 2. Collapsed Gibbs Sampling Update
For word $w_i$ in document $d$ assigned to topic $k$:

$$
P(z_i = k \mid z_{-i}, w) \propto
\frac{n_{d,k}^{-i} + \alpha}{\sum_{k'} (n_{d,k'}^{-i} + \alpha)} \cdot
\frac{n_{k,w_i}^{-i} + \beta}{\sum_{w'} (n_{k,w'}^{-i} + \beta)}
$$

Where:
- $n_{d,k}^{-i}$ = # of words in doc $d$ assigned to topic $k$ (excluding $i$).  
- $n_{k,w_i}^{-i}$ = # of times word $w_i$ assigned to topic $k$.  
- $\alpha, \beta$ = hyperparameters.

**Intuition:**
- First fraction: preference for topics already frequent in the document.  
- Second fraction: preference for topics strongly associated with the word.

---

### 3. Variational Inference vs Gibbs Sampling
- **Variational Inference:**  
  - Pros: Faster, scalable to large corpora.  
  - Cons: Biased approximation.  
- **Gibbs Sampling:**  
  - Pros: Asymptotically exact, easier to implement.  
  - Cons: Slower convergence, less scalable.

---

### 4. LDA Applied to NLP Research Papers
- **Insights:**  
  - Discover dominant themes (e.g., "machine translation," "question answering").  
  - Track topic trends over time.  
- **Limitations:**  
  - Bag-of-words assumption ignores word order.  
  - Topics may be incoherent or too coarse.  
  - Sensitive to hyperparameters and preprocessing.

<br>

## Part V. Comparative Analysis

### 1. HMM vs LDA
| Aspect | HMM | LDA |
|--------|-----|-----|
| Hidden variables | Sequence of hidden states | Document–topic assignments |
| Generative process | Sequential: states → emissions | Mixture: topics → words |
| Inference | Forward–Backward, Viterbi, EM | Gibbs sampling, Variational Bayes |
| Applications | POS tagging, speech recognition | Topic modeling, document clustering |

---

### 2. Common Challenges Across All Models
- **Data sparsity** (n-grams, HMM emissions).  
- **Simplifying assumptions** (Markov independence, bag-of-words).  
- **Scalability** to large corpora (LDA, HMM with long sequences).  
- **Interpretability** of hidden variables (topics, states).  
- **Generalization** across domains.
