# [Assignment Report] The Mathematical Foundations of Natural Language Processing


### **Problem 1: TF-IDF Calculation**

**Assignment:**
<br>
Consider the following corpus of three documents:
* **D1:** "the ship sailed on the sea"
* **D2:** "the sea is vast and deep"
* **D3:** "the ship was deep in the sea"

Calculate the TF-IDF score for the term "sea" in Document D1. Use the natural logarithm for the IDF calculation.

<br>

**Solution:**
<br>
The formula for TF-IDF is:
$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$
Where the Inverse Document Frequency (IDF) is calculated as:
$$
\text{IDF}(t) = \log\left(\frac{N}{|\{d \in D : t \in d\}|}\right)
$$
<br>

**Step 1: Calculate Term Frequency (TF)**
<br>
The term "sea" appears 1 time in Document D1, which has a total of 6 words.
$$
\text{TF}(\text{"sea"}, D1) = \frac{1}{6}
$$
<br>

**Step 2: Calculate Inverse Document Frequency (IDF)**
<br>
* Total number of documents in the corpus (N) = 3.
* Number of documents containing the term "sea" = 3 (D1, D2, D3).
<br>
$$
\text{IDF}(\text{"sea"}) = \log\left(\frac{3}{3}\right) = \log(1) = 0
$$
<br>

**Step 3: Calculate the Final TF-IDF Score**
<br>
$$
\text{TF-IDF}(\text{"sea"}, D1) = \text{TF}(\text{"sea"}, D1) \times \text{IDF}(\text{"sea"}) = \frac{1}{6} \times 0 = 0
$$
The TF-IDF score is 0. This indicates that the term "sea" is not a good differentiator for this corpus as it appears in every document.

<br>

---

<br>

### **Problem 2: The Chain Rule for Language Modeling**

**Assignment:**
<br>
The probability of a sequence of words $w_1, w_2, ..., w_n$ is given by the chain rule of probability:
$$
P(w_1, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})
$$
Explain the practical significance of this formula in the context of an NLP task such as machine translation or speech recognition. Why is this decomposition important?

<br>

**Solution:**
<br>
The chain rule is foundational to statistical language modeling. Its significance lies in decomposing the problem of calculating the joint probability of an entire sequence of words (which is computationally intractable to estimate directly) into a product of conditional probabilities.

In practice, a language model uses this rule to predict the next word given the previous words. For instance:
* **Machine Translation:** When generating a translated sentence, the model generates one word at a time. At each step, it calculates the probability of all possible next words given the sequence already generated. The chain rule provides the mathematical framework to score the likelihood of the entire generated sentence, helping the system choose the most probable (and thus, most fluent) translation.
* **Speech Recognition:** An audio signal can correspond to multiple possible word sequences (e.g., "recognize speech" vs. "wreck a nice beach"). The system uses a language model to calculate the prior probability of each candidate word sequence. The sequence with the higher probability according to the chain rule is considered more likely to be the correct transcription.

This decomposition is crucial because estimating $P(w_i | w_1, ..., w_{i-1})$ is far more feasible than estimating $P(w_1, ..., w_n)$ for the entire vocabulary and all possible sentence lengths.

<br>

---

<br>

### **Problem 3: Vector Space Models and Semantic Similarity**

**Assignment:**
<br>
In the Vector Space Model (VSM), documents are represented as vectors. Explain what the cosine similarity between two document vectors, $d_1$ and $d_2$, represents. If the cosine similarity between the vectors for "Artificial Intelligence" and "Machine Learning" is high (e.g., > 0.8), while the similarity between "Artificial Intelligence" and "Astrophysics" is low (e.g., < 0.2), what does this imply?

<br>

**Solution:**
<br>
The cosine similarity between two vectors measures the cosine of the angle between them. In the context of a VSM, it represents the **semantic similarity** of the documents. It is not concerned with the magnitude (e.g., length of the document) but rather the orientation of the vectors.

* A cosine similarity score close to 1 implies that the angle between the vectors is very small, meaning the documents are pointing in a similar direction in the vector space. This indicates a high degree of semantic or topical similarity.
* A cosine similarity score close to 0 implies that the vectors are nearly orthogonal, indicating they have very little topical overlap.
* A score close to -1 would imply they are opposites, which is rare in document analysis using non-negative weighting schemes like TF-IDF.

**Implication of the Example:**
<br>
A high cosine similarity (> 0.8) between "Artificial Intelligence" and "Machine Learning" implies that these two documents share a significant amount of related terminology and are considered topically very similar by the model. Conversely, the low similarity (< 0.2) between "Artificial Intelligence" and "Astrophysics" implies they share very few common terms and are topically distinct. This demonstrates the VSM's ability to capture thematic relationships within a corpus.