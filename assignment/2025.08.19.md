# Assignment: Categories of NLP Tasks



## Part I. Fundamental Language Processing

### 1. Definitions
- **Tokenization**: The process of breaking text into units (tokens), typically words, subwords, or characters. Example: *"The cat sat"* → ["The", "cat", "sat"].
- **Morphological Analysis**: Splitting words into morphemes (smallest meaning units) and assigning grammatical features. Example: *cats* → *cat (root, noun) + -s (plural suffix)*.
- **Part-of-Speech (POS) Tagging**: Assigning grammatical categories to words. Example: *cat/NOUN*, *sat/VERB*.

**Comparison of Roles**  
- Tokenization provides the basic input units.  
- Morphological analysis enriches tokens with structure.  
- POS tagging links tokens to syntactic categories.  
**Error propagation:** Mistakes in tokenization (splitting/joining errors) mislead morphology, which then misguides POS tagging, ultimately harming parsing, NER, and translation.

---

### 2. Morphologically Rich Languages
Example: **Korean**  
- Agglutinative morphology: words are formed by combining multiple morphemes.  
- Example:  
  - "먹었습니다" (*meogeotseumnida* = "ate")  
  - Decomposition: 먹/VerbStem + 었/PastTense + 습니다/PoliteEnding  
- Challenge: A single word may encode **tense, aspect, politeness, case marking**, leading to a large number of surface forms. English verbs (e.g., "eat/ate/eaten") show far fewer inflections.

---

### 3. Word-level vs Subword-level Tokenization
- **Word-level**  
  - Pros: Intuitive, aligns with human perception.  
  - Cons: Huge vocabulary, poor handling of rare/OOV words.  
- **Subword-level (e.g., Byte Pair Encoding, SentencePiece)**  
  - Pros: Compact vocabulary, handles rare words by decomposition (e.g., "internationalization" → "intern@@ ational@@ ization").  
  - Cons: Tokens may lose semantic transparency; longer sequences increase computational cost.  
**Modern NLP** prefers subword-level for balance of flexibility and efficiency.

<br>

## Part II. Semantic & Information Extraction

### 1. NER vs Relation Extraction
- **NER**: Identifies entities (e.g., *Barack Obama* as PERSON, *Paris* as LOCATION).  
- **Relation Extraction (RE)**: Identifies semantic relations between entities (e.g., *Barack Obama–born in–Hawaii*).  
**Importance:** Both are needed for **knowledge graphs**: NER provides nodes; RE provides edges.

---

### 2. Constituency vs Dependency Parsing
- **Constituency Parsing**: Represents phrases and hierarchical grouping (NP, VP).  
  - Useful in **text-to-speech** (prosody, phrasing) or **linguistic analysis**.  
- **Dependency Parsing**: Represents head–dependent word relations.  
  - Useful in **information extraction** and **relation identification**.  
**Example:**  
- Constituency more useful for **syntactic teaching tools** (showing phrase structures).  
- Dependency more useful for **extracting subject–verb–object triples**.

---

### 3. Figurative Language in Sentiment Analysis
- **Challenge**: Sarcasm reverses surface polarity ("Great, another delay").  
- **Strategies**:  
  - **Linguistic**: Incorporate pragmatic cues such as discourse markers, hyperbole.  
  - **Computational**: Use **context-aware models** (transformers trained with conversation history, multimodal signals like emoji or prosody in speech).

<br>

## Part III. Language Generation & Transformation

### 1. Extractive vs Abstractive Summarization
- **Extractive**: Selects and concatenates existing sentences/phrases.  
  - Challenge: Redundancy, lack of cohesion.  
- **Abstractive**: Generates new sentences that paraphrase the source.  
  - Challenge: Requires deeper semantic understanding; risk of hallucination.  
- **Closer to humans:** Abstractive summarization, since humans rephrase and condense rather than copy.

---

### 2. Machine Translation: Rule-based vs Neural
- **Rule-based**: Relies on hand-crafted grammars and lexicons.  
  - Limitation: Struggles with idioms and ambiguity.  
- **Neural (e.g., Transformers)**: Learns contextual mappings from large data.  
  - Example:  
    - Rule-based: "kick the bucket" → literal translation (*발로 양동이를 차다* in Korean).  
    - Neural: Correct idiomatic translation (*죽다*, "to die").  

---

### 3. QA vs Chatbot Systems
- **Question Answering**: Focused, fact-centric, single-turn. E.g., *“What is the capital of France?”* → *“Paris.”*  
- **Chatbot Dialogue**: Multi-turn, coherence-driven, often open-domain with personality.  
- **Difference:** QA emphasizes factual precision, while chatbots emphasize interaction flow and contextual memory.

<br>

## Part IV. Integrative & Critical Analysis

### 1. Intelligent Tutoring System for Language Learners
**Tasks integrated:**
- **Tokenization + POS tagging + Parsing** → grammatical feedback.  
- **NER** → vocabulary focus (highlight names, places).  
- **Sentiment analysis** → detect learner frustration.  
- **QA** → allow learners to query grammar rules.  
- **Dialogue system** → interactive practice.  

**High-level Architecture:**  
Input (student sentence) → Preprocessing (tokenization, POS, parse) → Error Detection Module → Feedback Generator → Conversational Interface with QA Support.

---

### 2. LLMs Blurring Task Boundaries
- **Example 1: ChatGPT-like models** perform both QA and summarization seamlessly without explicit task-specific modules.  
- **Example 2: Instruction-tuned LLMs** can do **translation, sentiment, and parsing** with the same model, unlike older specialized pipelines.

---

### 3. Bias in NLP Tasks
**Task:** Machine Translation  
- **Bias manifestation:** Gender-neutral languages (e.g., Turkish) often mapped to gender-stereotypical roles in English (e.g., “O bir doktor” → “He is a doctor”; “O bir hemşire” → “She is a nurse”).  
- **Mitigation:** Use **gender-neutral translation defaults** and allow user-specified gender options, combined with debiasing during fine-tuning.