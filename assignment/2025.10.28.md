## [Assignment Report] Advanced RNN Architectures

### ## Question 1: The Power of Hindsight in Bi-RNNs ðŸ¤”

**Part A:** Why is a standard, unidirectional RNN limited for many NLP tasks like Named Entity Recognition (NER)?

**Part B:** A Bi-RNN produces a forward hidden state $\overrightarrow{h_t}$ and a backward hidden state $\overleftarrow{h_t}$ at each time step `t`. Write the mathematical equation that combines these two states into the final hidden state representation $h_t$ for that time step. What does this combined vector represent?

---

### ## Solution 1

**Part A:** A standard RNN is limited because it processes a sequence in only one direction (left-to-right). This means its prediction for a word at position `t` is based only on **past context** ($x_1, \dots, x_t$). For many tasks, the meaning of a word is clarified by the words that come *after* it. A standard RNN has no access to this **future context**, which is a significant handicap.

**Part B:** The two states are combined using **concatenation**. The equation is:
$$
h_t = [\overrightarrow{h_t} ; \overleftarrow{h_t}]
$$
This combined vector $h_t$ represents a **rich summary of the word at time `t` that is informed by its entire context**. It contains information from all the words that came before it (via the forward state $\overrightarrow{h_t}$) and all the words that come after it (via the backward state $\overleftarrow{h_t}$).



***

### ## Question 2: Building Hierarchies with Deep RNNs ðŸ§±

**Part A:** What's the primary motivation for stacking RNN layers to create a Deep RNN? What kind of benefit does this hierarchical structure provide?

**Part B:** Write the general recurrence formula for the hidden state $h_t^{(l)}$ at an arbitrary layer $l > 1$ in a Deep RNN. What is the key difference in the input to this layer compared to the first layer ($l=1$)?

---

### ## Solution 2

**Part A:** The primary motivation is to learn a **hierarchy of features** over time. A single-layer RNN has to learn everything in one representation. A deep RNN can distribute this task:
* **Lower layers** might learn low-level features like syntax or parts of speech.
* **Higher layers** can take the sequence of representations from the lower layers and learn more abstract, high-level features like semantics, discourse, or long-range dependencies.

This hierarchical processing allows the model to learn more complex and nuanced patterns within the sequence data.

**Part B:** The general recurrence formula for a layer $l > 1$ is:
$$
h_t^{(l)} = f(W_{xh}^{(l)} h_t^{(l-1)} + W_{hh}^{(l)} h_{t-1}^{(l)} + b_h^{(l)})
$$
The key difference is the input.
* The **first layer** ($l=1$) takes the raw input sequence embeddings, $x_t$.
* Any **subsequent layer** ($l>1$) takes the **entire sequence of hidden states from the layer directly below it**, $h_t^{(l-1)}$, as its input.

***

### ## Question 3: Combining the Best of Both Worlds ðŸš€

When we create a Deep Bidirectional RNN, we stack Bi-RNNs on top of each other. What is the mathematical form of the input that layer $l+1$ receives from layer $l$ at each time step `t`?

---

### ## Solution 3

In a Deep Bi-RNN, each layer $l$ produces a sequence of forward hidden states $\overrightarrow{h_t}^{(l)}$ and backward hidden states $\overleftarrow{h_t}^{(l)}$. The complete representation for layer $l$ at time `t` is their concatenation, $h_t^{(l)} = [\overrightarrow{h_t}^{(l)} ; \overleftarrow{h_t}^{(l)}]$.

Therefore, the input that layer $l+1$ receives at time `t` is precisely this **sequence of concatenated hidden states from the layer below it**, $h_t^{(l)}$. This allows each level of the deep hierarchy to benefit from both past and future context.