# [Assignment Report] Foundations of Logistic Regression

### **Problem 1: The Sigmoid Function and Probability Estimation**

**Assignment:**
<br>
A logistic regression model is used for a binary classification task. For a particular data point $x$, the linear combination of the features and weights is calculated as $z = w^T x = 2$.

1.  Calculate the model's predicted probability that this data point belongs to the positive class, i.e., find $P(y=1|x) = \sigma(z)$. You may leave the answer in terms of the number $e$.
2.  If the classification threshold is 0.5, how would this data point be classified (as class 1 or class 0)? Explain your reasoning.

<br>

**Solution:**
<br>
**1. Probability Calculation:**
<br>
The predicted probability is calculated by passing the linear output $z$ through the sigmoid (logistic) function:
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
Substituting $z=2$:
$$
P(y=1|x) = \sigma(2) = \frac{1}{1 + e^{-2}}
$$
Numerically, this is approximately $1 / (1 + 0.135) \approx 0.88$.

<br>

**2. Classification Decision:**
<br>
The data point would be classified as **class 1**.
The reasoning is that the predicted probability, $P(y=1|x) \approx 0.88$, is greater than the classification threshold of 0.5. A key property of the sigmoid function is that $\sigma(z) > 0.5$ whenever $z > 0$. Since our $z=2$ is positive, the predicted probability must be greater than 0.5.

<br>

---

<br>

### **Problem 2: Binary Cross-Entropy Loss Calculation**

**Assignment:**
<br>
You are training a logistic regression model. The loss function for a single training example is the Binary Cross-Entropy (or Log Loss):
$$
J(w) = - \left[ y \log(\hat{y}) + (1-y) \log(1-\hat{y}) \right]
$$
where $y$ is the true label and $\hat{y}$ is the model's predicted probability. Use the natural logarithm ($\ln$) for your calculations.

1.  For one training example, the true label is $y=1$. The model confidently but incorrectly predicts $\hat{y} = 0.1$. Calculate the loss for this example.
2.  For another training example, the true label is $y=0$. The model again makes a confident but incorrect prediction of $\hat{y} = 0.9$. Calculate the loss for this example.
3.  Why is the loss value large in both of these cases?

<br>

**Solution:**
<br>
**1. Loss for the first example ($y=1, \hat{y}=0.1$):**
<br>
When the true label $y=1$, the loss formula simplifies to $J(w) = - \ln(\hat{y})$.
$$
J(w) = - \ln(0.1) \approx -(-2.302) = 2.302
$$
<br>

**2. Loss for the second example ($y=0, \hat{y}=0.9$):**
<br>
When the true label $y=0$, the loss formula simplifies to $J(w) = - \ln(1-\hat{y})$.
$$
J(w) = - \ln(1 - 0.9) = - \ln(0.1) \approx -(-2.302) = 2.302
$$
<br>

**3. Reason for High Loss:**
<br>
The loss is large in both cases because the Binary Cross-Entropy function heavily penalizes **confident, incorrect predictions**.
* In the first case, the event actually happened ($y=1$), but the model assigned a very low probability to it ($\hat{y}=0.1$).
* In the second case, the event did not happen ($y=0$), but the model assigned a very high probability to it happening ($\hat{y}=0.9$).
The logarithm term in the loss function approaches infinity as the predicted probability of the correct outcome approaches zero. This provides a strong gradient signal during training, forcing the model to significantly adjust its weights to correct such confident errors.

<br>

---

<br>

### **Problem 3: The Core Assumption of Logistic Regression**

**Assignment:**
<br>
What is the fundamental assumption that a Logistic Regression model makes about the relationship between the input features $x$ and the probability of the positive class? Express this assumption mathematically using the **logit** function.

<br>

**Solution:**
<br>
The fundamental assumption of Logistic Regression is that the **log-odds of the probability are a linear function of the input features**.

This means we don't assume the probability itself is linear with respect to the features, but rather a transformation of the probability is. The log-odds, or **logit** function, is this transformation.

Mathematically, this assumption is expressed as:
$$
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right) = w_0 + w_1x_1 + \dots + w_nx_n = w^T x
$$
where $p$ is the probability of the positive class, $p = P(y=1|x)$. This equation forms the core of the logistic regression model, linking the linear component $w^T x$ to the probability $p$ that we want to predict.