# [Assignment Report] Probabilistic N-gram Language Models

### **Problem 1: The Chain Rule and the Bigram Approximation**

**Assignment:**
<br>
Consider the sentence, "learning is always fun".
1.  Write the formula for the exact probability of this sentence, $P(\text{learning, is, always, fun})$, using the Chain Rule of Probability.
2.  Now, write the formula for the *approximated* probability of this sentence using a **bigram (N=2) model**. Assume the sentence is padded with a start `<s>` and end `</s>` token.

<br>

**Solution:**
<br>
**1. Exact Probability using the Chain Rule:**
<br>
The chain rule decomposes the joint probability of a sequence into a product of conditional probabilities, where each word is conditioned on all preceding words.
$$
P(\text{learning, is, always, fun}) = P(\text{learning}) \times P(\text{is} | \text{learning}) \times P(\text{always} | \text{learning, is}) \times P(\text{fun} | \text{learning, is, always})
$$
<br>

**2. Approximated Probability using a Bigram Model:**
<br>
A bigram model applies the Markov assumption, stating that the probability of a word depends only on the single preceding word. For the padded sentence `<s> learning is always fun </s>`, the probability is approximated as follows:
$$
P(\text{<s> learning is always fun </s>}) \approx P(\text{learning} | \text{<s>}) \times P(\text{is} | \text{learning}) \times P(\text{always} | \text{is}) \times P(\text{fun} | \text{always}) \times P(\text{</s>} | \text{fun})
$$
This can be written more compactly as:
$$
P(W) \approx \prod_{i=1}^{n+1} P(w_i | w_{i-1})
$$
where $w_0$ is `<s>` and $w_{n+1}$ is `</s>`.

<br>

---

<br>

### **Problem 2: Maximum Likelihood Estimation (MLE) for Bigrams**

**Assignment:**
<br>
Given the following small training corpus:
* `<s> the dog ran </s>`
* `<s> the cat ran </s>`
* `<s> the dog slept </s>`

Calculate the following bigram probabilities using Maximum Likelihood Estimation (MLE):
1.  $P_{MLE}(\text{dog} | \text{the})$
2.  $P_{MLE}(\text{ran} | \text{dog})$
3.  Based on this corpus, what is $P_{MLE}(\text{slept} | \text{cat})$? What is this problem known as in language modeling?

<br>

**Solution:**
<br>
The MLE formula for a bigram probability is:
$$
P_{MLE}(w_i | w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}
$$
First, we must tabulate the necessary counts from the corpus.
* **Unigram Counts:**
    * C(the) = 3
    * C(dog) = 2
    * C(cat) = 1
* **Bigram Counts:**
    * C(the, dog) = 2
    * C(the, cat) = 1
    * C(dog, ran) = 1
    * C(dog, slept) = 1
    * C(cat, slept) = 0
    * C(cat, ran) = 1

<br>

**1. Calculation of $P_{MLE}(\text{dog} | \text{the})$:**
<br>
$$
P_{MLE}(\text{dog} | \text{the}) = \frac{C(\text{the, dog})}{C(\text{the})} = \frac{2}{3}
$$
<br>

**2. Calculation of $P_{MLE}(\text{ran} | \text{dog})$:**
<br>
$$
P_{MLE}(\text{ran} | \text{dog}) = \frac{C(\text{dog, ran})}{C(\text{dog})} = \frac{1}{2}
$$
<br>

**3. Probability of an Unseen Bigram:**
<br>
$$
P_{MLE}(\text{slept} | \text{cat}) = \frac{C(\text{cat, slept})}{C(\text{cat})} = \frac{0}{1} = 0
$$
The model assigns a probability of zero to the bigram "cat slept" because this exact sequence never appeared in the training data. This is a critical flaw of the simple MLE approach and is known as the **zero-frequency problem** or the **sparsity problem**. It implies that any sentence containing this unseen (yet plausible) bigram would be assigned an overall probability of zero, which is an undesirable and often incorrect outcome.

<br>

---

<br>

### **Problem 3: Generalizing the MLE Formula**

**Assignment:**
<br>
State the general Maximum Likelihood Estimation (MLE) formula for an **N-gram model**. Then, write out the specific instance of this formula for a **trigram (N=3)** model, $P(w_i | w_{i-2}, w_{i-1})$. Explain what the numerator and the denominator represent.

<br>

**Solution:**
<br>
**1. General MLE Formula for an N-gram Model:**
<br>
The general MLE for the probability of a word $w_i$ given its preceding context of $N-1$ words is:
$$
P_{MLE}(w_i | w_{i-N+1}, \dots, w_{i-1}) = \frac{C(w_{i-N+1}, \dots, w_{i-1}, w_i)}{C(w_{i-N+1}, \dots, w_{i-1})}
$$
<br>

**2. Specific Formula for a Trigram (N=3) Model:**
<br>
For a trigram model, we set N=3. The formula becomes:
$$
P_{MLE}(w_i | w_{i-2}, w_{i-1}) = \frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})}
$$
* **Numerator: $C(w_{i-2}, w_{i-1}, w_i)$**
    * This term represents the **count of the full trigram** sequence "$w_{i-2} w_{i-1} w_i$" as it appears in the training corpus. It is the frequency of the event we are trying to predict (seeing $w_i$) occurring with its specific history.

* **Denominator: $C(w_{i-2}, w_{i-1})$**
    * This term represents the **count of the context or history**, which is the bigram "$w_{i-2} w_{i-1}$". It serves as the normalization factor, representing the total number of times the prefix (the conditioning event) has occurred in the corpus.