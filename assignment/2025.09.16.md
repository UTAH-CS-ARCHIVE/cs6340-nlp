# [Assignment Report] Vector Representations of Words (Word Embeddings)

### **Problem 1: Pointwise Mutual Information (PMI) Calculation**

**Assignment:**
<br>
You are creating word embeddings from a large corpus. After processing the corpus, you have gathered the following statistics:
* The total number of word co-occurrence pairs (tokens) in the context windows is 100 million.
* The word "deep" appears 20,000 times.
* The word "learning" appears 10,000 times.
* The words "deep" and "learning" co-occur within a context window 4,000 times.

Calculate the Pointwise Mutual Information (PMI) between "deep" and "learning". Use the base-2 logarithm ($\log_2$). What does the resulting value signify about the relationship between these two words?

<br>

**Solution:**
<br>
The formula for PMI is:

$$
\text{PMI}(w_1, w_2) = \log_2 \frac{P(w_1, w_2)}{P(w_1)P(w_2)}
$$

First, we must estimate the required probabilities from the given counts. Let $N$ be the total number of co-occurrence pairs ($10^8$).

* **Probability of "deep" ($P(\text{deep})$):**

    $$
    P(\text{deep}) = \frac{\text{Count}(\text{deep})}{N} = \frac{20,000}{100,000,000} = 2 \times 10^{-4}
    $$

* **Probability of "learning" ($P(\text{learning})$):**

    $$
    P(\text{learning}) = \frac{\text{Count}(\text{learning})}{N} = \frac{10,000}{100,000,000} = 1 \times 10^{-4}
    $$

* **Joint Probability of ("deep", "learning") ($P(\text{deep, learning})$):**

    $$
    P(\text{deep, learning}) = \frac{\text{Count}(\text{deep, learning})}{N} = \frac{4,000}{100,000,000} = 4 \times 10^{-5}
    $$

Now, we can substitute these probabilities into the PMI formula:

$$
\text{PMI}(\text{deep, learning}) = \log_2 \left( \frac{4 \times 10^{-5}}{(2 \times 10^{-4}) \times (1 \times 10^{-4})} \right) = \log_2 \left( \frac{4 \times 10^{-5}}{2 \times 10^{-8}} \right)
$$

$$
= \log_2 (2 \times 10^3) = \log_2(2000)
$$

Since $\log_2(1024) = 10$ and $\log_2(2048) = 11$, the value is approximately 10.97.

$$
\text{PMI}(\text{deep, learning}) \approx 10.97
$$

**Significance:** A large positive PMI value (significantly greater than 0) indicates that the words "deep" and "learning" co-occur far more frequently than would be expected if they were independent. This quantifies their strong semantic association.

<br>

---

<br>

### **Problem 2: The Role of Singular Value Decomposition (SVD)**

**Assignment:**
<br>
In count-based methods for generating word embeddings, a common approach is to first construct a high-dimensional, sparse Pointwise Mutual Information (PMI) matrix and then apply Singular Value Decomposition (SVD).

1.  What are the primary disadvantages of using the raw rows of the PMI matrix as word vectors?
2.  Explain the mathematical role of SVD in this process. What does it achieve, and what is the benefit of the resulting vectors?

<br>

**Solution:**
<br>
**1. Disadvantages of Raw PMI Vectors:**
* **High Dimensionality:** The PMI matrix has dimensions $|V| \times |V|$, where $|V|$ is the vocabulary size. This means each word vector is extremely high-dimensional (e.g., 50,000 dimensions), making them computationally expensive to store and use.
* **Sparsity and Noise:** The matrix is very sparse, with many zero entries for word pairs that never co-occur. The non-zero values can also be noisy and may not represent the most robust semantic relationships.
* **Lack of Generalization:** These raw vectors capture direct co-occurrence but may not generalize well to capture more nuanced, second-order relationships (e.g., two words being similar because they share similar contexts, even if they never appear together).

**2. The Role and Benefit of SVD:**
SVD is a matrix factorization technique that decomposes the PMI matrix $X$ into three matrices: $X = U \Sigma V^T$. Its role is to perform **dimensionality reduction** and find a **low-rank approximation** of the original matrix.

* **What it achieves:** By keeping only the top *k* singular values in the $\Sigma$ matrix (and the corresponding columns in *U* and *V*), we create an approximation $X_k = U_k \Sigma_k V_k^T$. This approximation is the best possible rank-*k* approximation of the original matrix in a least-squares sense.
* **The Benefit:** The word embeddings are taken from the rows of the truncated matrix $U_k$. The benefits of these new vectors are:
    * **Low-Dimensional and Dense:** They are now in a much smaller, fixed-size dimension *k* (e.g., 300 instead of 50,000) and are dense (no zero entries).
    * **Capturing Latent Semantics:** The SVD process forces the model to capture the most significant and robust patterns of co-occurrence while filtering out noise. The new dimensions correspond to latent semantic "topics" or concepts that group similar words together. This provides a much more powerful and generalized representation of word meaning.

<br>

---

<br>

### **Problem 3: Word2Vec Architectures**

**Assignment:**
<br>
Describe the fundamental difference between the **Continuous Bag-of-Words (CBOW)** and **Skip-gram** architectures in Word2Vec. For the example phrase "a model that learns word embeddings", illustrate this difference by stating the input and the prediction target for each model, assuming the target word is "learns".

<br>

**Solution:**
<br>
The fundamental difference lies in the **prediction task** that each model is trained to solve.

* **Continuous Bag-of-Words (CBOW):** The goal of CBOW is to predict a single target word based on its surrounding context words. It treats the context words as a "bag" (order doesn't matter) and uses their combined representation to predict the word in the middle.

* **Skip-gram:** The goal of Skip-gram is the inverse of CBOW. It takes a single target word as input and tries to predict the surrounding context words.

**Illustration:**
For the phrase "a model that learns word embeddings" and the target word "learns", with a context window of size 2:

* **CBOW:**
    * **Input:** The context words surrounding "learns" - `["model", "that", "word", "embeddings"]`
    * **Prediction Target:** The single word `"learns"`

* **Skip-gram:**
    * **Input:** The single word `"learns"`
    * **Prediction Target:** The set of context words - `{"model", "that", "word", "embeddings"}` (The model is trained to predict each of these context words from the input word).