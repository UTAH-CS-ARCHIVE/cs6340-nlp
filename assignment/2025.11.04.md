### [Assignment Report] The Attention Mechanism

**Question 1: Conceptual Foundation**

* Briefly explain the "information bottleneck" problem in the traditional Encoder-Decoder (Seq2Seq) architecture. How does the attention mechanism conceptually address this limitation?

<br>

**Question 2: Components of Attention**

* In the context of a Seq2Seq model translating a sentence, define the roles of the **Query (Q)**, **Key (K)**, and **Value (V)**. Specifically, what does each component typically represent (e.g., encoder hidden state, decoder hidden state)?

<br>

**Question 3: Mathematical Calculation of Scaled Dot-Product Attention**

* Given a single Query vector, a set of Key vectors, and a set of Value vectors, calculate the final context vector using the Scaled Dot-Product Attention formula. You must show each step of your calculation.

    * **Formula:**
    $$
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    $$

    * **Given values:**
        * The dimension of the key vectors, $d_k = 4$.
        * The Query matrix, $Q$:
        $$
        Q = \begin{bmatrix} 1 & 0 & 2 & 1 \end{bmatrix}
        $$
        * The Key matrix, $K$:
        $$
        K = \begin{bmatrix} 1 & 1 & 0 & 1 \\ 0 & 1 & 1 & 2 \end{bmatrix}
        $$
        * The Value matrix, $V$:
        $$
        V = \begin{bmatrix} 2 & 3 & 1 & 0 \\ 4 & 2 & 1 & 1 \end{bmatrix}
        $$
    
    * **Required Steps:**
        1.  Calculate the raw attention scores by computing $QK^T$.
        2.  Apply the scaling factor $\frac{1}{\sqrt{d_k}}$.
        3.  Compute the final attention weights by applying the softmax function to the scaled scores.
        4.  Calculate the final output (context vector) by multiplying the attention weights with the Value matrix $V$.

<br>
<br>

### **Report: Solutions for the Attention Mechanism Assignment**

**Solution to Question 1: Conceptual Foundation**

The **information bottleneck** in a standard Seq2Seq model arises because the encoder is forced to compress the entire information of an arbitrarily long input sequence into a single, fixed-length vector known as the context vector. For long sequences, this compression is lossy, and the model struggles to retain crucial details, especially from the beginning of the sequence. The decoder's performance is entirely dependent on this single vector, which often proves insufficient.

The attention mechanism addresses this by abandoning the single context vector. Instead, it allows the decoder to look back at the *entire sequence* of the encoder's hidden states at every decoding step. It calculates a unique context vector for each output step as a weighted sum of all encoder hidden states. The weights signify the "relevance" of each input part to the current output part, effectively allowing the model to focus on the most pertinent information from the source sequence, thus overcoming the bottleneck.

<br>

**Solution to Question 2: Components of Attention**

In a Seq2Seq model with attention:

* **Query (Q):** The Query is typically the **hidden state of the decoder** from the previous time step ($h_{t-1}^{dec}$). It represents the decoder's current state and is used to "ask" which parts of the input are most relevant for generating the next word.

* **Key (K):** The Keys are the **hidden states from the encoder** for all time steps ($h_1^{enc}, h_2^{enc}, \dots, h_N^{enc}$). Each key vector is paired with a value vector and acts as an identifier for the information contained in that value. The query is compared against each key to compute similarity scores.

* **Value (V):** The Values are also the **hidden states from the encoder**. In the original attention mechanism, the Key and Value vectors are often the same (i.e., $K=V$). The value vectors contain the actual information from the input sequence. The final context vector is a weighted sum of these value vectors, where the weights are determined by the query-key similarities.

<br>

**Solution to Question 3: Mathematical Calculation of Scaled Dot-Product Attention**

We are given:
$d_k = 4$, so the scaling factor is $\frac{1}{\sqrt{d_k}} = \frac{1}{\sqrt{4}} = \frac{1}{2} = 0.5$.

$$
Q = \begin{bmatrix} 1 & 0 & 2 & 1 \end{bmatrix}
$$
$$
K = \begin{bmatrix} 1 & 1 & 0 & 1 \\ 0 & 1 & 1 & 2 \end{bmatrix}
\implies
K^T = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 0 & 1 \\ 1 & 2 \end{bmatrix}
$$
$$
V = \begin{bmatrix} 2 & 3 & 1 & 0 \\ 4 & 2 & 1 & 1 \end{bmatrix}
$$

**Step 1: Calculate Raw Attention Scores ($QK^T$)**

$$
\text{Scores} = QK^T = \begin{bmatrix} 1 & 0 & 2 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 0 & 1 \\ 1 & 2 \end{bmatrix}
$$

$$
= \begin{bmatrix} (1 \cdot 1 + 0 \cdot 1 + 2 \cdot 0 + 1 \cdot 1) & (1 \cdot 0 + 0 \cdot 1 + 2 \cdot 1 + 1 \cdot 2) \end{bmatrix}
$$

$$
= \begin{bmatrix} (1 + 0 + 0 + 1) & (0 + 0 + 2 + 2) \end{bmatrix} = \begin{bmatrix} 2 & 4 \end{bmatrix}
$$

**Step 2: Apply the Scaling Factor**

$$
\text{Scaled Scores} = \frac{QK^T}{\sqrt{d_k}} = \begin{bmatrix} 2 & 4 \end{bmatrix} \cdot 0.5 = \begin{bmatrix} 1 & 2 \end{bmatrix}
$$

**Step 3: Compute Attention Weights (Softmax)**

$$
\text{Weights} = \text{softmax}(\begin{bmatrix} 1 & 2 \end{bmatrix}) = \left[ \frac{e^1}{e^1 + e^2}, \frac{e^2}{e^1 + e^2} \right]
$$

Using $e^1 \approx 2.718$ and $e^2 \approx 7.389$:
$$
e^1 + e^2 \approx 2.718 + 7.389 = 10.107
$$
$$
\text{Weights} = \left[ \frac{2.718}{10.107}, \frac{7.389}{10.107} \right] \approx \begin{bmatrix} 0.269 & 0.731 \end{bmatrix}
$$

**Step 4: Calculate the Final Output (Context Vector)**

$$
\text{Output} = \text{Weights} \cdot V = \begin{bmatrix} 0.269 & 0.731 \end{bmatrix} \begin{bmatrix} 2 & 3 & 1 & 0 \\ 4 & 2 & 1 & 1 \end{bmatrix}
$$

We calculate each component of the resulting vector:
* Component 1: $(0.269 \cdot 2) + (0.731 \cdot 4) = 0.538 + 2.924 = 3.462$
* Component 2: $(0.269 \cdot 3) + (0.731 \cdot 2) = 0.807 + 1.462 = 2.269$
* Component 3: $(0.269 \cdot 1) + (0.731 \cdot 1) = 0.269 + 0.731 = 1.000$
* Component 4: $(0.269 \cdot 0) + (0.731 \cdot 1) = 0 + 0.731 = 0.731$

The final context vector is:
$$
\text{Output} \approx \begin{bmatrix} 3.462 & 2.269 & 1.000 & 0.731 \end{bmatrix}
$$