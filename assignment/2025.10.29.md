## [Assignment Report] LSTMs and GRUs

### ## Question 1: The Heart of the LSTM - The Cell State Update ‚ù§Ô∏è

The equation for updating the cell state in an LSTM is a crucial part of its design:
$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$
Explain what this equation is doing. Specifically, describe the mathematical role of the **forget gate ($f_t$)** and the **input gate ($i_t$)** in controlling the flow of information.

---

### ## Solution 1

This equation describes how the LSTM's long-term memory, the **cell state ($C_t$)**, is modified at each time step. It's a two-part process of removing old information and adding new information.

1.  **Forgetting:** The term **$f_t \odot C_{t-1}$** handles what to forget. The **forget gate ($f_t$)** is a vector of values between 0 and 1. It performs an element-wise multiplication with the previous cell state ($C_{t-1}$). If a value in $f_t$ is close to 0, the corresponding information in $C_{t-1}$ is "forgotten" or discarded. If it's close to 1, the information is kept.

2.  **Adding:** The term **$i_t \odot \tilde{C}_t$** handles what new information to add. The **input gate ($i_t$)** acts as a filter, deciding which parts of the new "candidate" information ($\tilde{C}_t$) are important enough to be added to the cell state.

By combining these two steps, the LSTM can selectively erase old, irrelevant information and write new, relevant information to its memory at each time step.



***

### ## Question 2: The GRU's Elegant Simplicity üëç

The final hidden state update equation in a Gated Recurrent Unit (GRU) is:
$$
h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
$$
Explain the role of the **update gate ($z_t$)** in this equation. What kind of mathematical operation is this, and how does it allow the GRU to regulate its memory?

---

### ## Solution 2

The **update gate ($z_t$)** acts as a controller in a **convex combination** or interpolation.

The equation is essentially a weighted average that balances between the **previous hidden state ($h_{t-1}$)** and the **newly computed candidate state ($\tilde{h}_t$)**.

* The update gate $z_t$ is a vector of values between 0 and 1.
* If a value in $z_t$ is close to **1**, the model gives more weight to the new candidate state $\tilde{h}_t$ and less weight to the old state (since $(1-z_t)$ will be close to 0). This means it's **"updating"** its memory with new information.
* If a value in $z_t$ is close to **0**, the model gives more weight to the previous hidden state $h_{t-1}$ and mostly ignores the new candidate. This allows the model to **"preserve"** its old memory and carry it forward unchanged.

This simple interpolation mechanism allows the GRU to decide at each step whether to update its memory with new information or just carry over what it already knows.

***

### ## Question 3: How Do Gates Beat the Vanishing Gradient? üõ£Ô∏è

Both LSTMs and GRUs use "gating mechanisms" to solve the vanishing gradient problem that plagues simple RNNs. From a mathematical perspective, how does the structure of the cell/hidden state update in these models help gradients flow more effectively through time?

---

### ## Solution 3

The key is the **additive nature of the state updates**.

* In an LSTM, the cell state update is $C_t = f_t \odot C_{t-1} + \dots$
* In a GRU, the hidden state update is $h_t = (1-z_t) \odot h_{t-1} + \dots$

In both cases, the previous state ($C_{t-1}$ or $h_{t-1}$) is passed through a simple element-wise multiplication (the gate) and then **added** to the new information.

This is fundamentally different from a simple RNN, where the old state is multiplied by a weight matrix and then passed through a squashing non-linearity (like `tanh`), which can shrink gradients. The additive structure in LSTMs and GRUs creates a much more direct path for gradients to flow backward through time. The gates can learn to keep this path open (e.g., by setting the forget gate close to 1), allowing an uninterrupted **"gradient superhighway"** back through many time steps. This prevents the gradient from repeatedly being squashed by non-linearities, which is the primary cause of the vanishing gradient problem.