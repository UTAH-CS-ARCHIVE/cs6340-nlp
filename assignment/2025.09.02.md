# [Assignment Report] Information Theory for Language Model Evaluation

### **Problem 1: Entropy Calculation**

**Assignment:**
<br>
Consider a random variable representing a choice of programming language for a data science project. The probability distribution is as follows:
* $P(\text{Python}) = 0.6$
* $P(\text{R}) = 0.3$
* $P(\text{Julia}) = 0.1$

Calculate the entropy of this probability distribution. You may leave the answer in terms of logarithms.

<br>

**Solution:**
<br>
The formula for entropy $H(p)$ is:
$$
H(p) = -\sum_{x} p(x) \log_2 p(x)
$$
We apply this formula to the given probability distribution for the random variable *X* (choice of language).

$$
H(X) = - \left[ P(\text{Python})\log_2 P(\text{Python}) + P(\text{R})\log_2 P(\text{R}) + P(\text{Julia})\log_2 P(\text{Julia}) \right]
$$
<br>
Substituting the given probabilities:
$$
H(X) = - \left[ 0.6 \log_2(0.6) + 0.3 \log_2(0.3) + 0.1 \log_2(0.1) \right]
$$
<br>
Calculating the approximate numerical value:
$$
H(X) \approx - [0.6(-0.737) + 0.3(-1.737) + 0.1(-3.322)]
$$
$$
H(X) \approx - [-0.4422 - 0.5211 - 0.3322] = -[-1.2955] = 1.2955 \text{ bits}
$$
The entropy, or the average uncertainty, of the language choice is approximately 1.2955 bits.

<br>

---

<br>

### **Problem 2: Cross-Entropy Calculation**

**Assignment:**
<br>
Let's continue with the programming language example. The true probability distribution is $p = \{\text{Python}: 0.6, \text{R}: 0.3, \text{Julia}: 0.1\}$.
You have built a simple language model that predicts language usage, and it produces an estimated probability distribution $q = \{\text{Python}: 0.5, \text{R}: 0.4, \text{Julia}: 0.1\}$.

Calculate the cross-entropy $H(p, q)$ between the true distribution $p$ and your model's distribution $q$.

<br>

**Solution:**
<br>
The formula for cross-entropy $H(p, q)$ is:
$$
H(p, q) = -\sum_{x} p(x) \log_2 q(x)
$$
This measures the average number of bits needed to encode outcomes from the true distribution $p$ using a code optimized for the model's distribution $q$.

$$
H(p, q) = - \left[ p(\text{Python})\log_2 q(\text{Python}) + p(\text{R})\log_2 q(\text{R}) + p(\text{Julia})\log_2 q(\text{Julia}) \right]
$$
<br>
Substituting the probabilities from both distributions:
$$
H(p, q) = - \left[ 0.6 \log_2(0.5) + 0.3 \log_2(0.4) + 0.1 \log_2(0.1) \right]
$$
<br>
Calculating the numerical value:
$$
H(p, q) = - [0.6(-1) + 0.3(-1.322) + 0.1(-3.322)]
$$
$$
H(p, q) = - [-0.6 - 0.3966 - 0.3322] = -[-1.3288] = 1.3288 \text{ bits}
$$
The cross-entropy is 1.3288 bits. Since $H(p, q) > H(p)$ (1.3288 > 1.2955), this indicates that our model's distribution $q$ is not a perfect representation of the true distribution $p$. Our goal in training would be to adjust the model to minimize this cross-entropy value.

<br>

---

<br>

### **Problem 3: Perplexity Calculation and Interpretation**

**Assignment:**
<br>
A language model is evaluated on a test sentence $W = (w_1, w_2, w_3)$. The model assigns the following conditional probabilities to the words in the sequence:
* $P(w_1) = 1/4$
* $P(w_2 | w_1) = 1/2$
* $P(w_3 | w_1, w_2) = 1/8$

1.  Calculate the cross-entropy $H(p, q)$ of the model for this specific sequence.
2.  Calculate the perplexity (PPL) of the model on this sequence.
3.  Provide a brief, intuitive explanation of what the calculated perplexity value signifies.

<br>

**Solution:**
<br>
**1. Cross-Entropy Calculation:**
<br>
The cross-entropy for a sequence of length $N$ is the average negative log-likelihood:
$$
H(p, q) = -\frac{1}{N} \sum_{i=1}^{N} \log_2 q(w_i | w_1, \dots, w_{i-1})
$$
For this sequence, $N=3$.
$$
H(p, q) = -\frac{1}{3} \left( \log_2 P(w_1) + \log_2 P(w_2|w_1) + \log_2 P(w_3|w_1, w_2) \right)
$$
<br>
Substitute the given probabilities:
$$
H(p, q) = -\frac{1}{3} \left( \log_2(1/4) + \log_2(1/2) + \log_2(1/8) \right)
$$
<br>
Since $\log_2(1/4) = -2$, $\log_2(1/2) = -1$, and $\log_2(1/8) = -3$:
$$
H(p, q) = -\frac{1}{3} (-2 - 1 - 3) = -\frac{1}{3}(-6) = 2
$$
The cross-entropy is 2 bits.

<br>

**2. Perplexity Calculation:**
<br>
Perplexity is the exponentiation of the cross-entropy:
$$
\text{PPL} = 2^{H(p, q)}
$$
$$
\text{PPL} = 2^2 = 4
$$
The perplexity of the model on this sequence is 4.

<br>

**3. Interpretation of Perplexity:**
<br>
A perplexity of 4 means that, on average, the language model is as "surprised" or "confused" by the test sequence as it would be if it had to choose uniformly and independently from 4 equally probable options at each word. It represents the model's effective **branching factor**. A lower perplexity indicates a better model that is less uncertain about the correct next word.