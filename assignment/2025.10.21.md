# [Assignment Report] Recurrent Neural Networks and BPTT

### **Problem 1: The RNN Recurrence Formula and Parameter Sharing**

**Assignment:**
<br>
The core mathematical operation of a simple Recurrent Neural Network is defined by its recurrence formula:
$$
h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)
$$
Explain the specific roles of the weight matrices $W_{hh}$ and $W_{xh}$. What is the significance of these parameters being **shared** across all time steps in a sequence?

<br>

**Solution:**
<br>
**Roles of the Weight Matrices:**
* **$W_{xh}$ (Input-to-Hidden Weight Matrix):** This matrix is responsible for processing the current input. It transforms the input vector at the current time step, $x_t$, into the hidden state space. It determines how the new information from the current input will influence the network's memory.
* **$W_{hh}$ (Hidden-to-Hidden Weight Matrix):** This is the recurrent weight matrix. It is responsible for processing information from the past. It transforms the hidden state from the previous time step, $h_{t-1}$, which contains a summary of the sequence up to that point. It determines how the network's existing memory influences its new state.

**Significance of Parameter Sharing:**
<br>
Sharing the same set of parameters ($W_{hh}, W_{xh}, b_h$) across all time steps is a fundamental design principle of RNNs. This is significant for two main reasons:
1.  **Generalization:** It allows the network to learn a single set of rules for processing information that can be applied at any point in a sequence. A pattern learned from the beginning of a sequence can be recognized and processed in the same way if it appears at the end. This is crucial for generalizing across different temporal positions.
2.  **Handling Variable-Length Sequences:** By using the same parameters at each step, the model's architecture does not depend on the length of the input sequence. The total number of parameters is fixed, allowing a single trained RNN to process sequences of any length.

<br>

---

<br>

### **Problem 2: The Mathematical Core of Backpropagation Through Time (BPTT)**

**Assignment:**
<br>
During the training of an RNN using Backpropagation Through Time (BPTT), the gradient of the loss at a future time step $t$ with respect to a past hidden state $h_k$ (where $k<t$) is a critical component. This involves the partial derivative term $\frac{\partial h_t}{\partial h_k}$.

Write out the mathematical expression for this term as a product of Jacobians and explain what this product represents in the context of information flow in an RNN.

<br>

**Solution:**
<br>
**Mathematical Expression:**
<br>
The term $\frac{\partial h_t}{\partial h_k}$ is calculated by applying the chain rule recursively from time step $t$ back to time step $k$. This results in a product of Jacobian matrices:
$$
\frac{\partial h_t}{\partial h_k} = \frac{\partial h_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \cdots \frac{\partial h_{k+1}}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}
$$
<br>
**Explanation of what the product represents:**
<br>
This product represents the cumulative influence that the hidden state at a past time step $k$ has on the hidden state at a future time step $t$. Each term in the product, $\frac{\partial h_i}{\partial h_{i-1}}$, is the Jacobian of the recurrence function at step $i$. It describes how a small change in the hidden state at time $i-1$ is transformed into a change in the hidden state at time $i$.

By multiplying these Jacobians together, we are tracking the transformation of a signal (or gradient) as it flows forward through the network's recurrent connections over the time interval from $k$ to $t$. This product is therefore the mathematical formalization of how information (and gradients during BPTT) is propagated through the network's "memory" over time.

<br>

---

<br>

### **Problem 3: The Vanishing Gradient Problem in RNNs**

**Assignment:**
<br>
Using the product of Jacobians derived in the previous problem, provide a mathematical explanation for the **vanishing gradient problem**. How does this long product of matrices lead to the model's inability to learn long-range dependencies?

<br>

**Solution:**
<br>
The mathematical cause of the vanishing gradient problem lies directly in the repeated multiplication of the Jacobian matrix $\frac{\partial h_i}{\partial h_{i-1}}$ over many time steps, as shown in the expression $\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}$.

Each Jacobian matrix can be expressed as $\frac{\partial h_i}{\partial h_{i-1}} = \text{diag}(f'(z_i))W_{hh}^T$, where $f'$ is the derivative of the activation function and $W_{hh}$ is the shared recurrent weight matrix.

**Explanation of the Vanishing Effect:**
<br>
1.  The **vanishing gradient problem** occurs if the norm (or singular values) of this Jacobian matrix is consistently less than 1 for many consecutive time steps (i.e., $\| \frac{\partial h_i}{\partial h_{i-1}} \| < 1$).
2.  When this condition holds, the product of these matrices shrinks exponentially as the time gap ($t-k$) increases. For example, if the norm is consistently 0.9, then over 20 time steps, the magnitude of the gradient signal will be scaled by a factor of $0.9^{20} \approx 0.12$.
3.  This exponential decay causes the gradient signal from an error at a future time $t$ to become infinitesimally small by the time it is backpropagated to a much earlier time step $k$.
4.  As a result, the parameters of the network that were influential at time step $k$ (e.g., in processing a crucial early word in a sentence) receive virtually no update based on an error that occurs much later. This effectively "erases" the memory of distant past events from the learning process, making it impossible for the simple RNN to learn **long-range dependencies**.